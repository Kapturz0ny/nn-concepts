{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Transformer from scratch\n",
    "Heavily based on: https://arena-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch\n",
    "\n",
    "\n",
    "Other resource: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-11.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "\n",
    "import datasets\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float, Int\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "import torch\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_new(\n",
    "    input: Float[torch.Tensor, \"batch pos d_mlp\"], # ten string to zapisanie macierzy w sposób do użycia operacji einsum mnożenia mzacierzy\n",
    ") -> Float[torch.Tensor, \"batch pos d_mlp\"]:\n",
    "    # Implementation of GeLU used by GPT2 - subtly different from PyTorch's\n",
    "    return (\n",
    "        0.5\n",
    "        * input\n",
    "        * (\n",
    "            1.0\n",
    "            + torch.tanh(\n",
    "                np.sqrt(2.0 / np.pi) * (input + 0.044715 * torch.pow(input, 3.0))\n",
    "            )\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣ Understanding Inputs & Outputs of a Transformer\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> - Understand what a transformer is used for\n",
    "> - Understand causal attention, and what a transformer's output represents—algebra operations on tensors\n",
    "> - Learn what tokenization is, and how models do it\n",
    "> - Understand what logits are, and how to use them to derive a probability distribution over the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/transformer-overview-new.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens - Transformer Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair encodings\n",
    "\n",
    "\n",
    "We begin with the 256 ASCII characters as our tokens, and then find the most common pair of tokens, and merge that into a new token. Note that we do have a space character as one of our 256 tokens, and merges using space are very common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813641fccff845d0b1a50cc2119ff37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piotr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Piotr\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde367a4b6b543b5829f1d052991772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bdd31519fd4759a541d47f02330fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36ae9937f0945a4ad8e3cf96989ff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2506e874a5ac44668931e1162321999e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19648c0812484a219c8e22891c5a1cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf689bc8fa94489ac2dfaff1da4b443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "reference_gpt2 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(list(tokenizer.vocab.items()), key=lambda n: n[1])\n",
    "\n",
    "print(sorted_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Some tokenization annoyances\n",
    "\n",
    "There are a few funky and frustrating things about tokenization, which causes it to behave differently than you might expect. For instance:\n",
    "\n",
    "#### Whether a word begins with a capital or space matters!\n",
    "#### Arithmetic is a mess.\n",
    "\n",
    "Length is inconsistent, common numbers bundle together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'alph']\n",
      "['ĠRalph']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"Ralph\"))\n",
    "print(tokenizer.tokenize(\" Ralph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Key Takeaways\n",
    ">\n",
    "> * We learn a dictionary of vocab of tokens (sub-words).\n",
    "> * We (approx) losslessly convert language to integers via tokenizing it.\n",
    "> * We convert integers to vectors via a lookup table.\n",
    "> * Note: input to the transformer is a sequence of *tokens* (ie integers), not vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "Now that we understand the basic ideas here, let's go through the entire process of text generation, from our original string to a new token which we can append to our string and plug back into the model.\n",
    "\n",
    "#### **Step 1:** Convert text to tokens\n",
    "\n",
    "The sequence gets tokenized, so it has shape `[batch, seq_len]`. Here, the batch dimension is just one (because we only have one sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  40,  716, 1049, 3303, 2746,    0]], device='cuda:0')\n",
      "torch.Size([1, 6])\n",
      "I am great language model!\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am great language model!\"\n",
    "tokens = tokenizer(\n",
    "    reference_text, return_tensors=\"pt\", add_special_tokens=True\n",
    ").input_ids.to(device)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2:** Map tokens to logits\n",
    "\n",
    "\n",
    "From our input of shape `[batch, seq_len]`, we get output of shape `[batch, seq_len, vocab_size]`. The `[i, j, :]`-th element of our output is a vector of logits representing our prediction for the `j+1`-th token in the `i`-th sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = reference_gpt2(input_ids=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3:** Convert the logits to a distribution with a softmax\n",
    "\n",
    "This doesn't change the shape, it is still `[batch, seq_len, vocab_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "probs = logits.softmax(dim=-1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4:** Map distribution to a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = logits[0, -1].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5:** Add this to the end of the input, re-run\n",
    "\n",
    "There are more efficient ways to do this (e.g. where we cache some of the values each time we run our input, so we don't have to do as much calculation each time we generate a new value), but this doesn't matter conceptually right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence so far: I am great language model!\n",
      "7th char = ' I'\n",
      "8th char = ' am'\n",
      "9th char = ' a'\n",
      "10th char = ' great'\n",
      "11th char = ' teacher'\n",
      "12th char = '!'\n",
      "13th char = ' I'\n",
      "14th char = ' am'\n",
      "15th char = ' a'\n",
      "16th char = ' great'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence so far: {tokenizer.decode(tokens[0])}\")\n",
    "next_char = tokenizer.decode(next_token)\n",
    "for i in range(10):\n",
    "    print(f\"{tokens.shape[-1] + 1}th char = {next_char!r}\")\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n",
    "    # Pass our new sequence through the model, to get new output\n",
    "    logits = reference_gpt2(tokens).logits\n",
    "    # Get the predicted token at the end of our sequence\n",
    "    next_token = logits[0, -1].argmax(dim=-1)\n",
    "    # Decode and print the result\n",
    "    next_char = tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Key takeaways\n",
    "> \n",
    "> * Transformer takes in language, predicts next token (for *each* token in a causal way)\n",
    "> * We convert language to a sequence of integers with a tokenizer.\n",
    "> * We convert integers to vectors with a lookup table.\n",
    "> * Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).\n",
    "> * We append this to the input + run again to generate more text (Jargon: *autoregressive*)\n",
    "> * Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2️⃣ Clean Transformer Implementation\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream\n",
    "> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)\n",
    "> * Learn about & implement the following transformer modules:\n",
    ">     * LayerNorm (transforming the input to have zero mean and unit variance)\n",
    ">     * Positional embedding (a lookup table from position indices to residual stream vectors)\n",
    ">     * Attention (the method of computing attention patterns for residual stream vectors)\n",
    ">     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
    ">     * Embedding (a lookup table from tokens to residual stream vectors)\n",
    ">     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level architecture\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-new2.png\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Embedding\n",
    "\n",
    "The input tokens $t$ are integers. We get them from taking a sequence, and tokenizing it (like we saw in the previous section).\n",
    "\n",
    "The token embedding is a lookup table mapping tokens to vectors, which is implemented as a matrix $W_E$. The matrix consists of a stack of token embedding vectors (one for each token)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual stream\n",
    "\n",
    "The residual stream is the sum of all previous outputs of layers of the model, and is the input to each new layer. It has shape `[batch, seq_len, d_model]` (where `d_model` is the length of a single embedding vector).\n",
    "\n",
    "The initial value of the residual stream is denoted $x_0$ in the diagram, and $x_i$ are later values of the residual stream (after more attention and MLP layers have been applied to the residual stream).\n",
    "\n",
    "The residual stream is *really* fundamental. It's the central object of the transformer. It's how model remembers things, moves information between layers for composition, and it's the medium used to store the information that attention moves between positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer blocks\n",
    "\n",
    "Then we have a series of `n_layers` **transformer blocks** (also sometimes called **residual blocks**).\n",
    "\n",
    "Note - a block contains an attention layer *and* an MLP layer, but we say a transformer has $k$ layers if it has $k$ blocks (i.e. $2k$ total layers).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-block2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "First we have attention. This moves information from prior positions in the sequence to the current token.\n",
    "\n",
    "We do this for *every* token in parallel using the same parameters. The only difference is that we look backwards only (to avoid \"cheating\"). This means later tokens have more of the sequence that they can look at.\n",
    "\n",
    "Attention layers are the only bit of a transformer that moves information between positions (i.e. between vectors at different sequence positions in the residual stream).\n",
    "\n",
    "Attention layers are made up of `n_heads` heads - each with their own parameters, own attention pattern, and own information how to copy things from source to destination. The heads act independently and additively, we just add their outputs together, and back to the stream.\n",
    "\n",
    "Each head does the following:\n",
    "* Produces an **attention pattern** for each destination token, a probability distribution of prior source tokens (including the current one) weighting how much information to copy.\n",
    "* Moves information (via a linear map) in the same way from each source token to each destination token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-attn-new-v2.png\" width=\"1050\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "The MLP layers are just a standard neural network, with a singular hidden layer and a nonlinear activation function. The exact activation isn't conceptually important ([GELU](https://paperswithcode.com/method/gelu) seems to perform best).\n",
    "\n",
    "Our hidden dimension is normally `d_mlp = 4 * d_model`. Exactly why the ratios are what they are isn't super important (people basically cargo-cult what GPT did back in the day!).\n",
    "\n",
    "Importantly, **the MLP operates on positions in the residual stream independently, and in exactly the same way**. It doesn't move information between positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-mlp-new-2.png\" width=\"680\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unembedding\n",
    "\n",
    "Finally, we unembed!\n",
    "\n",
    "This just consists of applying a linear map $W_U$, going from final residual stream to a vector of logits - this is the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### LayerNorm\n",
    "\n",
    "* Simple normalization function applied at the start of each layer (i.e. before each MLP, attention layer, and before the unembedding)\n",
    "* Converts each input vector (independently in parallel for each `(batch, seq)` residual stream vector) to have mean zero and variance 1.\n",
    "* Then applies an elementwise scaling and translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Code!\n",
    "\n",
    "Model architecture table (this will be helpful for understanding the results you get when running the code block below):\n",
    "\n",
    "| Parameter   | Value          |\n",
    "|-------------|----------------|\n",
    "| batch       | 1              |\n",
    "| position    | 6             |\n",
    "| d_model     | 768            |\n",
    "| n_heads     | 12             |\n",
    "| n_layers    | 12             |\n",
    "| d_mlp       | 3072 (= 4 * `d_model`) |\n",
    "| d_head      | 64 (= `d_model / n_heads`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "\n",
    "LayerNorm should do the following:\n",
    "\n",
    "* Make mean 0\n",
    "* Normalize to have variance 1\n",
    "* Scale with learned weights\n",
    "* Translate with learned bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(\n",
    "        self, residual: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (\n",
    "            residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps\n",
    "        ).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The input tokens $t$ are integers. We get them from taking a sequence, and tokenizing it (like we saw in the previous section).\n",
    "\n",
    "The token embedding is a lookup table mapping tokens to vectors, which is implemented as a matrix $W_E$. The matrix consists of a stack of token embedding vectors (one for each token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: Int[Tensor, \"batch position\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embedding\n",
    "Positional embedding can also be thought of as a lookup table, but rather than the indices being our token IDs, the indices are just the numbers `0`, `1`, `2`, ..., `seq_len-1` (i.e. the position indices of the tokens in the sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: Int[Tensor, \"batch position\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(\n",
    "            self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "\n",
    "* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (including current token)\n",
    "    * Linear map from input -> query, key shape `[batch, seq_posn, head_index, d_head]`\n",
    "    * Dot product every *pair* of queries and keys to get attn_scores `[batch, head_index, query_pos, key_pos]` (query = dest, key = source)\n",
    "    * **Scale** and mask `attn_scores` to make it lower triangular, i.e. causal\n",
    "    * Softmax along the `key_pos` dimension, to get a probability distribution for each query (destination) token - this is our attention pattern!\n",
    "* **Step 2:** Move information from source tokens to destination token using attention pattern (move = apply linear map)\n",
    "    * Linear map from input -> value `[batch, key_pos, head_index, d_head]`\n",
    "    * Mix along the `key_pos` with attn pattern to get `z`, which is a weighted average of the value vectors `[batch, query_pos, head_index, d_head]`\n",
    "    * Map to output, `[batch, position, d_model]` (position = query_pos, we've summed over all heads)\n",
    "\n",
    "Note - when we say **scale**, we mean dividing by `sqrt(d_head)`. The purpose of this is to avoid vanishing gradients (which is a big problem when we're dealing with a function like softmax - if one of the values is much larger than all the others, the probabilities will be close to 0 or 1, and the gradients will be close to 0).\n",
    "\n",
    "Below is a much larger, more detailed version of the attention head diagram from earlier. This should give you an idea of the actual tensor operations involved. A few clarifications on this diagram:\n",
    "\n",
    "* Whenever there is a third dimension shown in the pictures, this refers to the `head_index` dimension. We can see that all operations within the attention layer are done independently for each head.\n",
    "* The objects in the box are activations; they have a batch dimension (for simplicity, we assume the batch dimension is 1 in the diagram). The objects to the right of the box are our parameters (weights and biases); they have no batch dimension.\n",
    "* We arrange the keys, queries and values as `(batch, seq_pos, head_idx, d_head)`, because the biases have shape `(head_idx, d_head)`, so this makes it convenient to add the biases (recall the rules of array broadcasting!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/transformer-attn-30.png\" width=\"1400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head))) # to są weights\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head))) # to są biaas\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\n",
    "            \"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device) # IGNRORE to maskowanie -inf\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"] # mnożenie przy pomocy einsum jest przydatny, bo nie trzeba się zastanawiać jak resizować macierzy, ani transponować\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # Calculate query, key and value vectors\n",
    "        q = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_Q,\n",
    "                \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\", # to jest przetwarzanie macierzy przy pomocy einsum\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        k = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_K,\n",
    "                \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        v = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre,\n",
    "                self.W_V,\n",
    "                \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q,\n",
    "            k,\n",
    "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\",\n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v,\n",
    "            attn_pattern,\n",
    "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\",\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = (\n",
    "            einops.einsum(\n",
    "                z,\n",
    "                self.W_O,\n",
    "                \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = t.ones(\n",
    "            attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device\n",
    "        )\n",
    "        mask = t.triu(all_ones, diagonal=1).bool() # górna macierz trójkątna\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "the MLP layer consists of:\n",
    "\n",
    "* A linear layer, with weight `W_in`, bias `b_in`\n",
    "* A nonlinear function (we usually use GELU; the function `gelu_new` has been imported for this purpose)\n",
    "* A linear layer, with weight `W_out`, bias `b_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        pre = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_mid,\n",
    "                self.W_in,\n",
    "                \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        post = gelu_new(pre)\n",
    "        mlp_out = (\n",
    "            einops.einsum(\n",
    "                post,\n",
    "                self.W_out,\n",
    "                \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerBlock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre # Skip connection\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unembed\n",
    "\n",
    "The unembedding is just a linear layer (with weight `W_U` and bias `b_U`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab))) # Mapowanie na słownik embedding do słowa\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "                \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
    "            )\n",
    "            + self.b_U\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DemoTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: Int[Tensor, \"batch position\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3️⃣ Training a Transformer\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand how to train a transformer from scratch\n",
    "> * Write a basic transformer training loop\n",
    "> * Interpret the transformer's falling cross entropy loss with reference to features of the training data (e.g. bigram frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config( # Trochę mniejszy model, żeby szybciej się trenował\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab=tokenizer.vocab_size + 1,  # PAD\n",
    ")\n",
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Args\n",
    "\n",
    "\n",
    "Note, for this optimization we'll be using **weight decay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs: # Mały trening\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    max_steps_per_epoch = 200\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ea9e63323043c486bd121945eab1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piotr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Piotr\\.cache\\huggingface\\hub\\datasets--NeelNanda--pile-10k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314f21fee5bc4cf3a2ce8e83e1904f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a7acd3ec6247fab6e1b2584994268d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-4746b8785c874cc7.parquet:   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0df0a336aa4ac5bd10a523bc9e30eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playi\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\n",
    "    \"meta\"\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset[0][\"text\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8176fd423b534f81bb1159777e57165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # First tokenize without BOS token\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=model_cfg.n_ctx - 1,  # Leave room for BOS token\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    # Add BOS token at the beginning of each sequence\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    num_sequences = len(input_ids)\n",
    "    bos_tokens = torch.full((num_sequences, 1), tokenizer.bos_token_id)\n",
    "    tokenized[\"input_ids\"] = torch.cat([bos_tokens, input_ids], dim=1)\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"input_ids\", \"tokens\")\n",
    "tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset_dict[\"test\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we iterate through these dataloaders, we will find dictionaries with the single key `'tokens'`, which maps to a tensor of token IDs with shape `(batch, seq_len)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens', 'attention_mask'])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "print(first_batch.keys())\n",
    "print(first_batch[\"tokens\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = (\n",
    "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    )\n",
    "\n",
    "    return log_probs_for_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41861ca136594b688eefbfa473d1e5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8933a6fad949cc90c0855c611efd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88cf9fee113406aadd901e4d8a1e961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7becd053104fc4a61453c1e59a8b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b088999fed154f25b29d3eef93781e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88d673c78fe47398e6d22596ba2c947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee13005b95164a7e9ac74882e6c00cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d9401b5b454b9fb052b3760b2f79c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1c0550d4de4f52938058792c2df55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2db7fb3cf204e589f1a6c950b5112eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96885cadcb142cabf1ae00e290e6b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7410e88130064028a159858f01763bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0c65bc63404ab3994055e9adfa63c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167a25f8c02e4d46910e5446a841a188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b99bd2060c4361a26393f7f3bccb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37694bdb01a743d2b9ace72b12912002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378d1ed342d741328ec7c8e1a93acc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b186ac2e182488cb6b89e79da5e77a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a3bfc73b574c9ea78b702dd797340c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0bddf1659d4f4bbfa593f40463b759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadafe55a79e4d2b8de3f596b9a547e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "        self.optimizer = t.optim.AdamW(\n",
    "            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "        )\n",
    "        self.step = 0\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset_dict[\"train\"],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            dataset_dict[\"test\"],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.train_losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: dict[str, Int[Tensor, \"batch seq\"]]\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
    "\n",
    "        Remember that `batch` is a dictionary with the single key 'tokens'.\n",
    "        \"\"\"\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits = self.model(tokens)\n",
    "        loss = -get_log_probs(logits, tokens).mean()\n",
    "        loss.backward()\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.step += 1\n",
    "        return loss\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set and return the accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_correct, total_samples = 0, 0\n",
    "\n",
    "        for batch in tqdm(self.test_loader, desc=\"Evaluating\"):\n",
    "            tokens = batch[\"tokens\"].to(device)\n",
    "            logits: Tensor = self.model(tokens)[:, :-1]\n",
    "            predicted_tokens = logits.argmax(dim=-1)\n",
    "            total_correct += (predicted_tokens == tokens[:, 1:]).sum().item()\n",
    "            total_samples += tokens.size(0) * (tokens.size(1) - 1)\n",
    "\n",
    "        accuracy = total_correct / total_samples\n",
    "        return accuracy\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
    "        for each epoch at `self.args.max_steps_per_epoch` steps.\n",
    "        \"\"\"\n",
    "        accuracy = np.nan\n",
    "\n",
    "        progress_bar = tqdm(total=self.args.max_steps_per_epoch * self.args.epochs)\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, batch in enumerate(self.train_loader):\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(\n",
    "                    f\"Epoch {epoch + 1}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\"\n",
    "                )\n",
    "                if i >= self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "            accuracy = self.evaluate()\n",
    "            self.accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAnndJREFUeJzt3Qd4U9X7wPG3uxTaQtmFsvfeG0FBAVFBRQEHuAfiQv0rKqCiorgHP1AUt4ILcQKCgKjsvfceZbalLXTm/5wDCUmbtEmb9GZ8P89zybq5ORn03veec943yGQymQQAAAAAALhdsPs3CQAAAAAAFIJuAAAAAAA8hKAbAAAAAAAPIegGAAAAAMBDCLoBAAAAAPAQgm4AAAAAADyEoBsAAAAAAA8h6AYAAAAAwEMIugEAAAAA8BCCbsCL3XbbbVKrVq0iPfe5556ToKAgt7cJAAAUjdovq/2zq/bu3auf++mnn3qkXQA8i6AbKAK143NmWbhwoQTqyYIyZcoY3QwAAPJRgat5P/3PP//ke9xkMklCQoJ+/KqrrhJf9fvvv+v3EB8fL7m5uUY3BwhooUY3APBFX3zxhc3tzz//XP7888989zdu3LhYrzN16tQi7yifffZZeeqpp4r1+gAA+KvIyEj5+uuvpVu3bjb3L1q0SA4ePCgRERHiy7766is9Wk71kv/111/Su3dvo5sEBCyCbqAIbrnlFpvbS5cu1UF33vvzSk9Pl6ioKKdfJywsrMhtDA0N1QsAAMjvyiuvlO+++07effddm/2lCsTbtm0rJ06cEF+VlpYms2bNkgkTJsgnn3yiA3BvDbpVW0uXLm10MwCPYng54CE9e/aUZs2ayapVq+SSSy7RwfbTTz+tH1M7wv79++shX+pMet26dWX8+PGSk5NT4Jxu85yu119/XT788EP9PPX89u3by4oVKwqd061ujxw5Un766SfdNvXcpk2byuzZs/O1Xw2Nb9eune4JUK/zwQcfuH2euDrYUQc2pUqVkgoVKuiTFocOHbJZ5+jRo3L77bdL9erVdXurVq0qAwYM0J+F2cqVK6VPnz56G2pbtWvXljvuuMNmO2rEwNtvv63fr3pPlStXlnvvvVdOnz5ts54z2wIA+L6hQ4fKyZMn9Ulzs8zMTPn+++/lpptuchggPvbYY3r4udonNWzYUO+T1ZB0axkZGfLoo49KxYoVJTo6Wq655hrde26P2u+p/YzaL5n3y9OmTSvWe5s5c6acPXtWbrjhBhkyZIj8+OOPcu7cuXzrqfvUvr1BgwZ636j2sdddd53s2rXLZv/5zjvvSPPmzfU66j317dtX7y8Lm2+edw67+Thi8+bN+jMuV66cZaTB+vXr9XFPnTp19OtUqVJFfy7qO7L3md15552W4yi1r77//vv197d79279Gm+99Va+5/3333/6sW+++aYYny7gOrrBAA9SO4p+/frpHZ4KKNUOVVE7JjXnedSoUfpSDfsaO3aspKSkyGuvvVbodtVZ+DNnzuigUe08Jk6cqHeSakdTWO+4mr+mdr4jRozQBwLqDP/1118v+/fvl/Lly+t11qxZo3eoauf7/PPP65MBL7zwgt7Ruov6DFQwrU4YqDPxiYmJeqf+77//6tcvW7asXk+1bdOmTfLggw/qExDHjh3TB0iqvebbV1xxhW6bGk6vnqcOANR7tKY+K/NrPvTQQ7Jnzx55//339Wup11Sfm7PbAgD4PrUP6dy5sw7A1L5a+eOPPyQ5OVnvt9X+0ZoKrFXwvGDBAh3wtWrVSubMmSNPPPGEDgKtg7y77rpLvvzySx1YdunSRe/n1cn2vNS+r1OnTpaT4mr/o9qgtq+OCR555JEivTfVs33ppZfqwFW9F7VP++WXX3QQbqb27WrO+vz58/U6Dz/8sD62UPvYjRs36hPuimqL2n+qz0i9r+zsbFm8eLEe5adOzheFakf9+vXl5ZdftpywUK+rjmPUflq1W+37VQeDulSvZT7pf/jwYenQoYMkJSXJPffcI40aNdKfvzpZokYUqqC9a9eu+jNQJz7yfi7q2EedvAdKlAlAsT3wwANqj2FzX48ePfR9U6ZMybd+enp6vvvuvfdeU1RUlOncuXOW+4YPH26qWbOm5faePXv0NsuXL286deqU5f5Zs2bp+3/55RfLfePGjcvXJnU7PDzctHPnTst969at0/e/9957lvuuvvpq3ZZDhw5Z7tuxY4cpNDQ03zbtUe0uXbq0w8czMzNNlSpVMjVr1sx09uxZy/2//vqr3v7YsWP17dOnT+vbr732msNtzZw5U6+zYsUKh+ssXrxYr/PVV1/Z3D979myb+53ZFgDAt33yySeWv/Xvv/++KTo62rJfvuGGG0yXXnqpvq72v/3797c876efftLPe/HFF222N2jQIFNQUJBl37p27Vq93ogRI2zWu+mmm/T9av9sduedd5qqVq1qOnHihM26Q4YMMcXGxlraZd7/q7YXJjExUe+vp06darmvS5cupgEDBtisN23aNL3NN998M982cnNz9eVff/2l13nooYccrlNQ2/K+X/OxydChQ506Nvrmm2/0+n///bflvmHDhpmCg4Pt7qvNbfrggw/087Zs2WJz7FGhQgV9jAKUNIaXAx6khjypM7Z5qWHLZuqsspo31r17d32GduvWrYVud/DgwXpIlpl6rqLOEBdGzekyn71WWrRoITExMZbnqjPf8+bNk4EDB+phW2b16tWz9AQUlxqSpnqVVW+7GkJmpnoB1Bnr3377zfI5hYeH66HueYeBm5l7xH/99VfJyspyOIw9NjZWLr/8cv1Zmxc1tF2NNFC9Fs5uCwDgP2688UY9DFv93Vf7Y3XpaGi5ygYeEhKiR0tZU8PNVXypeqjN6yl518vba62e88MPP8jVV1+tr1vvn9Q0J9Xjvnr1apff0/Tp0yU4OFiPFLMeSq/aZ70vVa+tplKpkWR5mXuV1Trq+rhx4xyuUxT33XdfgcdGati7+hzUKADF/Dmooe5qipz6zOz1spvbpL5XdXyherbN1KgEtc3C8u8AnkDQDXhQtWrVdNCYlxoqde211+pAUAW8ajiZeSegdrKFqVGjhs1tcwDuKDAt6Lnm55ufq4JhdQCiguy87N1XFPv27dOXai5cXiroNj+uTlq8+uqr+kBBDc1Xc+PVUHo1z9usR48e+sBCDYNXBw9qyJhKGqPm05nt2LFDf66VKlXSn7X1kpqaqt+zs9sCAPgPtR9QJ6PVtC01lUideB40aJDdddW+SZ2MVsOT7VUqMe+71KUKeq1PcNvb5x0/flwPkVZDqPPum8wn7M37J1eoYe1q+LWa4rZz5069tG7dWs93ViehzdS8bdWmgpKuqnXUe46LixN3UnOw8zp16pQe4q729yoAV5+DeT3zsZH6zNSwe5WXpiDqJLoKzNX3aqYCcHVcdtlll7n1vQDOYE434EHWZ23N1A5WBXcq2FbzpNVOWZ2NVWdxn3zySadKhKkz7fbkTeTi7ucaQfUMqB2nOrOtzlKPGTNGzwFX8+PUQYQ6q63mcan5Xmq+mlpHJV5544039H2qJ1t9pirgtj7jbc08V92ZbQEA/Ivq2b777rv1CV01oss86snTzPt7ddJ9+PDhdtdRo9FcoU4ymxOrqjnTean9oJoH7U6OerzzJoct7PhI9U6rRGdqjryaL2/ef6scM0Upnzps2DB9kkFtUyWB+/nnn/UIO3VCBChpBN1ACVNDpdXZZ3VGXfXcmqnEXt5ABafqJIA6M56XvfuKombNmvpy27Zt+c44q/vMj5upExNq+J5a1AGF2hmrQFidzTdTQ9DU8tJLL+kz2zfffLMeYqeSvqjnqyHzKrGKvR19XgVtCwDgX9TIM5VsU51cnTFjhsP11L5J7UvUMHTr3m7ztDDzvktdqiDR3JNsvX+zZs5sroJTd5XzUkG1Sgz6xRdf5DvJrhKpquRwKhGpGvWm9o3Lli3T06kcJWFV66gT0KoX2lFvt3m0nepUsGbu+XeGGm2nErqpkWYqsayZ2ufn/cxUp4VK9FYYFayr9dVn0rFjRz2F79Zbb3W6TYA7caoHKGHmnaB1z7Ia8vW///1PvKV9auevepZVhlDrgNs8X6241DwsFdxPmTLFZui22v6WLVssGV7VDjJviRN1AKAOUszPUzvqvL30KihXzOuos+fqoEaVZctLZWE1Hyg4sy0AgH9RPaqTJ0/W5azUyKqC6nqrfYmqfGFNZS1Xvb3mvCfmy7zZz1XZyrz7WzWlSc2bthdEqqHUrlIBpsrzonK/qGHy1ovqQVbM5bLUa6s5znnfj2LeF6p11HUVDDtaRwXBakrW33//bfO4K8c19o6N7H1mqpda5ZxRo9HMJcvstUlRw+bVXPZvv/1WZ19Xvd2ujhwA3IWebqCEqdIh6qywGkqmkqyoHbU6I+1Nw7vVgcfcuXN1z7Cqe2k+yFBzqNauXevUNtSZ8xdffDHf/epMuRrepeZqqzlraqi92imaS4apEi7mEh/bt2+XXr166aC5SZMmegeqao+qdVV5E+Wzzz7TO3bVU6ECctUDMXXqVH0QoA6QFPUaqhdDDUtX7VdlwdRZfXUGXQ09U6+rDkic2RYAwP84Gt5tTQXkqgzXM888o8tJtmzZUu8rZ82apadCmedwq5O1ar+m9idqLrLa76teXHujxV555RWdzFP1xKoh7mpfp3qV1ZQz1auurjtL9Vqr11Clx+xR85nbtGmjA3M1nU0Nv/788891+dLly5frYF3VIVevq/bTKq+Jer+qd1idQFD7TPNQb1UyTD1mfi01Eky9F3WpTqyrAFztw52l9rPmvC3q+EG1VX229kYBqjJj6jG1b1dD5dWc+iNHjuj9uerNt54eoN6jarv6jNVxB2CYEs+XDgRQybCmTZvaXf/ff/81derUyVSqVClTfHy86f/+7/9Mc+bM0dtYsGBBoSXD7JXQclSWI+86qq15qdfIW0Jj/vz5ptatW+sSY3Xr1jV99NFHpscee8wUGRlZ6OehtqVey96itmU2Y8YM/RoRERGmuLg4080332w6ePCg5XFVQkW1t1GjRroEmSqf0rFjR9O3335rWWf16tW69EiNGjX0dlQpsquuusq0cuXKfO368MMPTW3bttWfuyoR07x5c/3ZHz582OVtAQB8v2RYQfKWDFPOnDljevTRR/W+OywszFS/fn29TzaXqjJT5TBVmS1V4lPtv1QpzgMHDuTbV5tLfKl9XUJCgt5mlSpVTL169dL7LDNnSoY9+OCDep1du3Y5XOe5557T66hyoeYyXc8884ypdu3altdWJdCst5Gdna3fo9oXq2OCihUrmvr162datWqVZR21HVX+TO2n1f71xhtvNB07dszhscnx48fztU3t/6+99lpT2bJl9XZU+Ta1f7b3me3bt0+XDlNtUfvrOnXq6M8wIyMj33bVsZgqMWZ9fAGUtCD1j3EhPwBfooZ0qczreedYAQAAeCOVdFWNslOjDQCjMKcbgF2qbJg1FWir2qM9e/Y0rE0AAADOUvO+1bQyNcwcMBI93QDsqlq1qtx2221Sp04dnYFUJZlRycTWrFljtwwJAACAN1CJ6VatWqUrnahkcbt379aVWQCjkEgNgF0qWYrKcKrqlkZEREjnzp118hICbgAA4M2+//57eeGFF3TJNnUsQ8ANo9HTDQAAAACAhzCnGwAAAAAADyHoBgAAAADAQ/x+Tndubq4cPnxYoqOjJSgoyOjmAACgqdldZ86ckfj4eAkO5hx4QdiXAwB8eV/u90G32kknJCQY3QwAAOw6cOCAVK9e3ehmeDX25QAAX96X+33Qrc6KK+qDiImJMbo5AABoKSkpOpA076fgGPtyAIAv78v9Pug2D0NTO2l21AAAb8Nw6cKxLwcA+PK+nElkAAAAAAB4CEE3AAAAAAAeQtANAAAAAICHEHQDAAAAAOAhBN0AAAAAAHgIQTcAAAAAAB5C0A0AAAAAgIcQdAMAAAAA4CEE3QAAAAAAeAhBNwAAAAAAHkLQDQAAAACAhxB0AwAAAADgIQTdAAAAAAB4CEE3AAAAAAAeEuqpDfujBduOSXpGjnSpW17KlQ43ujkAAAAAvFBmdq5sTzwjh5LOSkRosESFh0qpsBApFX5hCQuRqPAQ/VhQUJDRzYWHEXS7YMxPG+Xg6bPy44guBN0AAAAAJCsnV3YkpsrGQ8my/lCSbDiYLFuOntGBtzPMAXjkhaDccv3C/eoyUt2vbkeESsPK0dKqRlmJj40kYPcRBN0uMP+mTSajWwIAAAAEHpPJJFk5JsnOzZXsXJOUDg+VkOCSCzyzc3Jl1/E0WX8w6UKQnSybD6dIhp0AOyYyVGpXLKOfczYzR85m5Uj6hUvrgFzdVourKpSJkFYJZaV1jbLSsnpZaZEQKzGRYcV+j3A/gm4XBIn5PzRRNwAAAODKNM0fVx/SwacOmHNU8Hw+cFZBqSWQVvdbHjflWzcn1/Y4XMXbcaUjpEKZcKkYHSEVy0RIhejzt1VQalmiw6V86QiXAnT1WruPp8oGFVwfTNaXKsC2FyBHR4RKs2qx0qJ6rOWyRlyUw55otW0dbGfmyDmrYPx8cJ4tZzNzL9zOtgnWk9OzZOPhZNl65IycSM2QeVsS9WJWt2JpaZVQTveEt6peVhpVjZawENJ4GY2g2wXm/6P0dAMAAACFO5p8Tl74dZP8vuGoR7avYnAVfKpl69EzBa6r4t+4qPPBeEXrwFxfP387KT1LB9dqiLgKblWwm1fp8BBpqgLrarHSvHqsNK8WK7XKl5ZgFwJ6FfyXiQjVS1GoQH3T4WRZsz9J1h1MlrUHTsuBU2d1L7xaflh9UK+n5oyrkwCqJ9wciCfElWJYegkj6HaB+ceZ5wQbAAAAgDw9uZ8v2StvzN0uqRnZOsi8tVNNaVglWkKDg3Tva2hIkIQGB0uYugwJlrDg85fq/rDgYP0c82PWz1GPqUv1eMrZLDmug+5MOX7mfPB9wnyZmmkJyE+mZeqOM3Wplm2JBQfoZmo+dbNqMZbe6+bVykqdCq4F2J6g5ny3rRmnF7OTqRmy7mCSrN2fJGsPJsu6A0mSfDZLVu07rRf59/x65UuHS8uEspZAvG3NckUO/uEcPl0XWAaX09UNAAAA2KXmOz89c4NsPJSib6s5xy8NbC5N4mM8EnxWioksdD01hP1UeqacOHMxELcE5mcydOCugnaVuKxF9bKWILtuxTIlOme8OMqXiZDLGlXWizlm2XsyXfeCmwPxLYdT9EmHv7Ye04sSGRYsfZpWkevbVJeu9Sr4zPv1JQTdrjAPLze6HQAAAICXSTmXJW/M2SafL92ne5VVIrEn+zWSoe1rGN4zrHrLK0VH6iWQRunWrlBaL9e2rq7vy8jOkS1Hzsja/adl7YEkWbX//LD0WWsP66VKTKQMbF1NBrWtJvUqRRv9FvwGQXeReroNbggAAADgJVSPqpqz/fwvm+TYmQx938BW8fJM/yZ67jS8R0RoiM54rhbzd6eCbzUH/Jd1R+RoyjmZsmiXXlpWj5Xr21aXq1vEUy65mAi6XRB8YU63ib5uAAAAQPafTJcxszbKou3H9W3Vqzp+QDPpVr+C0U2Dk73hrWuU08uYq5rIX1uO6QB8wbbjOkGbWsb/ull6NaqsA/CeDSuSDb0ICLpdQJ1uAAAAQHSd6amLd8u783foGtXhIcFyf8+6elHzrOGbveD9mlfVi5rvroab/7DqoGw+kiKzNx3VS1zpcLmmZbwMaltdmsbHkAXdSQTdRajTTdANAACAQLVs90l55qeNsvNYqr7dpW55GT+wmU46Bv+gSqjd2a22XrYcSdHB909rD+tg/NP/9uqlYeVoub5tNRnYqppTyewCGUF3UXq6GV4OAACAAHMqLVMm/L5Fvlt10FJ66tmrGuugix5P/9W4aow8e1UTeapfI/l7x3H5YdUh+XNzoi679vLvW+WVP7bKJQ0q6uznlzepzEgHOwi6i4CebgAAAAQKlWxLBdoq4D6dnqXvu6ljDXmyTyOJjQozunkowQzw5pJkyelZ8sv6w/Lj6oOyen+SLNx2XC/RkaFyeePKujxcoyoxui57RZLpEXQXLZEaAAAA4P92JJ7RQ8mX7zmlbzeqEi0vXdtc2tYsZ3TTYCB1suWWTjX1svt4qvy4+pAOwA8nn5Mf1xzSi1n50uE6+FaL+v00rBIjDSqXkajwwAlFA+eduoF51EwuXd0AAADwY2czc+T9BTvkw793S1aOSUqFhcijl9eX27vWJns1bNSpWEYe79NQRl3eQJbuPilL95ySbUdTZNvRM7LvVLqcTMuU/3ad1It1XFUjLkrPCzcH4ioor1U+Sveo+xuCbhdYpqoQcwMAAMCP5OaadJbq/3adkH93npQVe09JemaOfqx340ry3DVNpXq5KKObCS8WHBwkXepV0ItZema27EhM1QH41qNnZFvi+WD8RGqm7DuZrpe5mxMt64eHBkv9SmVsesXrViyt71ejjlU4pvIHBAddSHIdpEYjn79PPabXuRCzma9b329U7gGC7qJkLyfqBgAAgI/P0957Ml3+3XlCB9pLdp20zNc2q1a2lIy9uon0aVrFsHbCt6kh5C0TyurFmsqCbgnEL/SKb09MlbNZObLpcIpePKl9rXLy3X1dpKQQdLuAOt0AAH83adIkee211+To0aPSsmVLee+996RDhw521/3xxx/l5Zdflp07d0pWVpbUr19fHnvsMbn11lttDuzHjRsnU6dOlaSkJOnatatMnjxZrwugZB1LOSf/XujJVkH2oaSzNo+XDg+RDrXjpKvqraxbQfc0qt5LwBMlySrUi9C/NevRFvtPpV8IxM/3iqvrqjc8J9e9AVhJx3ME3S4wD0cg6AYA+KMZM2bIqFGjZMqUKdKxY0d5++23pU+fPrJt2zapVKlSvvXj4uLkmWeekUaNGkl4eLj8+uuvcvvtt+t11fOUiRMnyrvvviufffaZ1K5dW8aMGaMf27x5s0RGUtcV8KTks1m6praaS6t6tHdcqKttFhYSJK1rlJOudStI13rldW8k87VhlODgIKlVobRe+jbLP7pCncRVcZjpwnUVh6sRyObYLNfO42K1jn78QiwXWsInk4JMqkV+LCUlRWJjYyU5OVliYmKKta2Bk/6VtQeS5MNb28oVDLMBAHjJ/sldVKDdvn17ef/99/Xt3NxcSUhIkAcffFCeeuopp7bRpk0b6d+/v4wfP14f9MTHx+ve78cff1w/rt5v5cqV5dNPP5UhQ4b47GcFeKNzWTmyat9pHWD/u+ukbDiYdD7wuED1HzWNj9FBtpp3q4bYBlIGacDdnN0/8b+sKMPLjW4IAABulpmZKatWrZLRo0db7gsODpbevXvLkiVLCn2+CrD/+usv3Sv+6quv6vv27Nmjh6mrbZipgxMV3KttOgq6MzIy9GJ9UAPAsaPJ5+TZnzbK3zuOS2Z2rs1jdSqUli71yutAu1Od8lKudLhh7QQCFUG3CyzJy4m6AQB+5sSJE5KTk6N7oa2p21u3bnX4PHV2v1q1ajpIDgkJkf/9739y+eWX68dUwG3eRt5tmh+zZ8KECfL8888X8x0BgUHVzx7x1WqdmEqpHBNh6cnuUre8xJctZXQTgYBH0O2CiynmiboBAFCio6Nl7dq1kpqaKvPnz9dzwuvUqSM9e/Ys8jZVb7vajnVPtxrmDsB2dMkXS/fJC79sluxck0569saNLaVJ1RjDyiIBsI+g2wXm+fb0dAMA/E2FChV0T3Vi4sV6qYq6XaWK4zwmagh6vXr19PVWrVrJli1bdE+1CrrNz1PbqFq1qs021bqORERE6AWA47nbz8zcKD+sPqhvX90yXl69vjnzswEvRXrCItTpdnPGegAADKeyj7dt21b3VpupRGrqdufOnZ3ejnqOeT62ylauAm/rbape62XLlrm0TQAXqTJfN0xZogNu1SH0bP/G8u6QVgTcgBfjf6crLInUiLoBAP5HDekePny4tGvXTtfmViXD0tLSdBkwZdiwYXr+turJVtSlWrdu3bo60P7999/liy++0HW4FTXE9ZFHHpEXX3xR1+U2lwxTGc0HDhxo6HsFfNF/u07IyK/XyKm0TCkXFSaTbmqj524D8G4E3S4gkRoAwJ8NHjxYjh8/LmPHjtWJztQQ8NmzZ1sSoe3fv18PJzdTAfmIESPk4MGDUqpUKV2v+8svv9TbMfu///s/vd4999wjSUlJ0q1bN71NanQDrs3f/vifPTLhj62Sk2vSZb8+uLWtVC8XZXTTADiBOt0uGPLhElm6+5S8O7S1XNMy3m1tBAAEHmpPO4/PCoHsbGaOPPXjepm19rC+fV3ravLydc0lMizE6KYBAS+FOt3uF3whE6Sfn6cAAACAFzhwKl3u/WKVbD6SIiHBQTKmf2MZ3qUW2ckBH0PQ7QLz3zdibgAAAHjS4h3H5cFv1khSepaULx0uk25uI53qlDe6WQCKgKC7CNnLSaQGAAAAT1AjKj/4e7dMnL1VV8xpWT1WJt/SVuLLljK6aQCKiKDbBfR0AwAAwFPSM7Plie/Xy2/rj+jbN7StLuMHNmP+NuDjCLqLgKAbAAAA7rT3RJqev70t8YyEhQTJ2Kubyi0dazB/G/ADBN1FSaRmdEMAAADgNxZsOyYPf7NGUs5lS8XoCJl8cxtpVyvO6GYBcJOLxTYN8Pfff8vVV18t8fHx+izeTz/9lG9Oi6oVWrVqVV3/s3fv3rJjxw7D2ms+0ZhLVzcAAACKSR3rTlqwU+74dIUOuNvUKCu/PtiNgBvwM4YG3WlpadKyZUuZNGmS3ccnTpwo7777rkyZMkWWLVsmpUuXlj59+si5c+fECJbBPcTcAAAAKIbUjGy578tV8tqcbXrq4k0da8g393SSyjGRRjcNgD8NL+/Xr59eHJ35e/vtt+XZZ5+VAQMG6Ps+//xzqVy5su4RHzJkiAE93WQvBwAAQPHsP5kud3y2QnYeS5XwkGB5YUBTGdKhhtHNAuCPPd0F2bNnjxw9elQPKTeLjY2Vjh07ypIlSxw+LyMjQ1JSUmwWd/d0M7ocAAAARXHgVLoM+XCJDrgrx0TIjHs7EXADfs5rg24VcCuqZ9uaum1+zJ4JEybo4Ny8JCQkeKCnGwAAAHDNwdPpMnTqUjmcfE7qVCwtP4/sJq1rlDO6WQACNeguqtGjR0tycrJlOXDggNu2TSI1AAAAFMWR5LNy09RlcvD0WalVPkq+uZv520Cg8Nqgu0qVKvoyMTHR5n512/yYPRERERITE2OzuAvDywEAAOCqxJRzMvTDpbL/VLrUiIsiYRoQYLw26K5du7YOrufPn2+5T83PVlnMO3fubEibzD3dxNwAAABwxrEz5/SQ8r0n06Va2VLy9d0dpWpsKaObBSBQspenpqbKzp07bZKnrV27VuLi4qRGjRryyCOPyIsvvij169fXQfiYMWN0Te+BAwca0t4gc183Xd0AAAAoxInUDLl56jLZfTxN4mMjZfo9naR6uSijmwUgkILulStXyqWXXmq5PWrUKH05fPhw+fTTT+X//u//dC3ve+65R5KSkqRbt24ye/ZsiYw0ZjgOPd0AAABwxqm0TLnlo2Wy41iqVImJlK/v7iQJcQTcQCAyNOju2bOnrsddULbwF154QS/eINicvZyoGwAAAA4kpZ8PuLcePSOVoiP0kPJaFUob3SwABvHaOd1eiezlAAAAKEByepbc8vEy2XwkRSqUCdc93HUqljG6WQAMRNDtArKXAwAAwJGUc1kybNoy2XgoReJKnw+461Ui4AYCHUG3C9Rwd4WYGwAAANZSM7LltmnLZd3BZCkbFSZf3dVRGlSONrpZALwAQXeReroJuwEAAHBeWka23P7Jclm9P0liS4XJl3d2lMZVY4xuFgAvQdDtgmBz1A0AAACISHpmttzx6QpZsfe0REeG6oC7WbVYo5sFwIsQdBdheDmJ1AAAAHAuK0fu+mylLNtzSspEhMrnd3SQ5tUJuAHYIuh2AYnUAAAAYA647/58pfy366SUDg+Rz+5oL61rlDO6WQC8EEF3EaJuYm4AAIDAlZGdI/d/uUoW7zghpcJC5JPbO0jbmnFGNwuAlyLodkHQhaibnm4AAIDAlJmdKw98tVoWbDsukWHBMu229tKhNgE3AMcIuouQSM1EXzcAAEDAycrJlQe/WS3zthyTiNBg+Xh4e+lct7zRzQLg5Qi6XXAhjxo93QAAAAEmOydXHpm+VuZsSpTwkGD5cFg76VqvgtHNAuADCLqLNLycqBsAACBQ5OSaZNS36+S3DUckLCRIPri1rfRoUNHoZgHwEaFGN8CX0NMNAAAQWJLPZsn/fb9O93CHBgfJ5JvbyqWNKhndLAA+hKC7KEG30Q0BAACAx63ad1oe+maNHEo6qwPu929qLb2bVDa6WQB8DEG3C4IuRN30dAMAAPj3cPIpi3bJm39u19drxEXJu0NbS6uEskY3DYAPIuh2wYWObskl6gYAAPBLiSnn5NEZa+W/XSf17WtaxstL1zaT6Mgwo5sGwEcRdLuA4eUAAAD+a8HWY/LYd+vkVFqmlAoLkRcGNJVBbatbRjsCQFEQdLsg2DK8nLAbAADAX2Rk58jE2dvk43/26NtNqsbIeze1lroVyxjdNAB+gKC7SEG30S0BAACAO+w5kSYPfrNaNh5K0bdv61JLnurXSCLDQoxuGgA/QdDtAvPIohyibgAAAJ/34+qDMuanjZKWmSPlosLktUEtyU4OwO0Iul0QciHqJpEaAACA70rNyNbB9sw1h/TtjrXj5J0hraVKbKTRTQPghwi6XRAczPByAAAAX7bhYLIeTr73ZLqoQ7tHejeQBy6tJyEXjvMAwN0Iul2QmZ2rL3cdSzW6KQAAAHBBbq5Jpv27R16dvVWyckxSrWwpeWdIK2lXK87opgHwcwTdLvj0v736cv7WY0Y3BQAAAE46kZohj3+3ThZuO65v921aRV69voXERlF7G4DnBZfAawAAAB8xadIkqVWrlkRGRkrHjh1l+fLlDtedOnWqdO/eXcqVK6eX3r1751s/MTFRbrvtNomPj5eoqCjp27ev7NixowTeCXDevztPSL93FuuAOyI0WF4c2Ewm39KGgBtAiSHoBgAA2owZM2TUqFEybtw4Wb16tbRs2VL69Okjx47ZH+G1cOFCGTp0qCxYsECWLFkiCQkJcsUVV8ihQ+eTU5lMJhk4cKDs3r1bZs2aJWvWrJGaNWvq4DwtLa2E3x0CTVZOrkycvVVu+XiZHD+TIfUrlZGfR3aTWzrVlCBzSRoAKAFBJrVH9GMpKSkSGxsrycnJEhMTU6xt1XrqN8v1va/0d0PrAACByp37J3dRPdvt27eX999/X9/Ozc3VgfSDDz4oTz31VKHPz8nJ0T3e6vnDhg2T7du3S8OGDWXjxo3StGlTyzarVKkiL7/8stx1110++1nBux04lS4PTV8ja/Yn6ds3dawhY/o3kVLh1N4G4D7O7p/o6QYAAJKZmSmrVq3SvdBmwcHB+rbqxXZGenq6ZGVlSVzc+cRUGRkZ+lINVbfeZkREhPzzzz9ufw+AciT5rAyY9K8OuKMjQ+V/N7eRl69tTsANwDAE3S54ok9DfVmOOUAAAD9z4sQJ3VNduXJlm/vV7aNHjzq1jSeffFLP3TYH7o0aNZIaNWrI6NGj5fTp0zqwf/XVV+XgwYNy5MgRh9tRwbrqPbBeAGfk5Jrk0Rlr5VRapjSqEi2/P9Rdrmxe1ehmAQhwBN0uSIiL0peNqjC0DQAAa6+88opMnz5dZs6caenZDgsLkx9//FEPM1e93yqRmpr/3a9fP93j7ciECRP0cD3zooa4A8744O9dsnT3KYkKD5HJt7S1HLsBgJEIul0QfCHnRq5/T4MHAASgChUqSEhIiM42bk3dVnOwC/L666/roHvu3LnSokULm8fatm0ra9eulaSkJN27PXv2bDl58qTUqVPH4fZUz7iaH2deDhw4UMx3h0Cw7kCSvDl3u77+3NVNpXaF0kY3CQA0gm4XBMn5qJuQGwDgb8LDw3WAPH/+fMt9KumZut25c2eHz5s4caKMHz9eB9Pt2rVzuJ7qsa5YsaIuF7Zy5UoZMGCAw3XVnG+VkMZ6AQqSlpEtj8xYK9m5JrmyeRW5oV11o5sEABahF6/C2Z5uP0/4DgAIUKpc2PDhw3Xw3KFDB3n77bd1aa/bb79dP64yklerVk0P/1bU/OyxY8fK119/rWt7m+d+lylTRi/Kd999p4NtNbd7w4YN8vDDD+syYqq0GOAuz/+ySfacSJOqsZEy4doWlAQD4FUIul1g/gOeS8wNAPBDgwcPluPHj+tAWgXQrVq10j3Y5uRq+/fvt5mLPXnyZJ0cbdCgQTbbUXW+n3vuOX1dDSlXwbwapl61alUduI8ZM6aE3xn82e8bjsi3Kw+KOkx7a3AriSXhLQAvQ51uF8zddFTu+WKVtK5RVmaO6Oq2NgIAAg+1p53HZwVHDiedlb5v/y0p57LlgUvryhN9GhndJAABJIU63e4XTE83AACAV5UHUwF3y4Sy8kjvBkY3CQDsIuh2gXlEnZ8PDgAAAPB6UxbtkmV7Tknp8BB5Z3ArCQvhsBaAd+KvUxHmdBNzAwAAGGftgSR5688L5cGuaSq1KA8GwIsRdBdpeDlRNwAAgBFSM7Ll4elrdHmw/i2qyqC2lAcD4N0Iul1gLj7BnG4AAABjPPfzJtl3Ml3iYyPl5YHNKQ8GwOsRdBehp5s53QAAACXv1/WH5ftVByWY8mAAfAhBtwvUH3iF4eUAAAAl61DSWRn94wZ9fUTPetKxTnmjmwQATiHodoF5+BLDywEAAEq4PNj0tXLmXLa0SigrD/eub3STAMBpBN1F6OlmeDkAAEDJmbxwpyzfe6E82BDKgwHwLfzFckHwhaibmBsAAKBkrNl/Wt6at0Nff35AM6lZnvJgAHwLQbcLmNMNAABQ0uXB1urh5Ve1qCrXt6lmdJMAwGUE3S5gTjcAAEDJGTdrk+w/lS7VypaSl66lPBgA30TQXYSSYfR0AwAAeNYv6w7LD6utyoOVojwYAN9E0O0C87lVYm4AAADPOXg6XZ6eeb482MhL60mH2nFGNwkAioyguwg93apOJAAAANxPzd8eNWOdLg/WukZZeagX5cEA+DaCbhdER4ZarlM2DAAAwP3+t+B8ebAyEaHyzuDWEkp5MAA+jr9iLqgQHWG5npGda2hbAAAA/M3q/afl7fnny4O9MKCp1CgfZXSTAKDYCLpdUCosxHL9bGaOoW0BAADwJ2fOZcnD09fo4eXXtIyXa1tTHgyAfyDodkGIuVC3mm/E8HIAAAC3lgc7cOqsLg/24rXNKA8GwG8QdLvI/PefmBsAAMA9Zq09JD+uOaTLg70zpJXERFIeDID/IOguctkwom4AAIDiOnAqXZ6duVFfH3lZfWlXi/JgAPwLQbeLzEOdCLkBAACKJzsnVx6dsVbOZGRLG1Ue7LJ6RjcJANyOoNtF5mnddHQDAAAUz6QFu2TlvtPny4MNoTwYAP/EXzYXBV0YYJ5L1A0AAFBkq/adlnf/Ol8ebPzAppIQR3kwAP6JoNtV5p5uo9sBAADgw+XBHplxvjzYgFaqPFh1o5sEAB5D0O0iEqkBAAAUz9gL5cGqlysl4wc2M7o5AOBRBN0uCjYnUiPmBgAAKFJ5sJlrDklIcBDlwQAEBIJuF1GnGwAAoPjlwR68rJ60rUl5MAD+j6C7qMPLmdUNAADgUnmwh6ev0eXB2tUsJyMvpTwYgMBA0F3E4eW5xNwAAABOe++vnbJ6f5JER4TKW4NbUR4MQMDgr52rLMPLiboBAACcsXLvKXnvQnmwF69tRnkwAAGFoLvIw8sBAABQmJRzWfLw9LV6lOC1ravJgFbVjG4SAJQogm4XBQeTvRwAAMBZY37aKIeSzkpCXCl5YUBTo5sDACWOoNtF1OkGAABwzsw1B2XW2sO6PNjbg1tLNOXBAAQggm4XBZnrdBvdEAAAAC+2/2S6jPlpk77+0GX1pW3NckY3CQAMQdDtogujyxleDgAAUEB5sEdmrJHUC+XBHri0rtFNAgDDEHS7zFwyjKgbAADAnnfN5cEiQ+XtIZQHAxDY+Avoogujy+npBgAAsGPF3lPy/oXyYC9d21yql6M8GIDARtBd1OHlYpIcVfsCAAAAWvLZLHnkQnmw69pUk2taxhvdJAAwHEG3i4IuDC//34Jd0mTsbNl4KNnoJgEAABhOVXZ59kJ5sBpxUfLCgGZGNwkAvIJXB905OTkyZswYqV27tpQqVUrq1q0r48ePN7Rcl3l4+W8bjkhGdq48/8v5rJwAAACB7MfVh+SXdefLg70zpJWUiQg1ukkA4BW8+q/hq6++KpMnT5bPPvtMmjZtKitXrpTbb79dYmNj5aGHHjKkTcHmqBsAAADavpNpMnbWRn39kV71pXUNyoMBgE/0dP/3338yYMAA6d+/v9SqVUsGDRokV1xxhSxfvtywNqkhUwAA+KtJkybpfW5kZKR07NixwH3u1KlTpXv37lKuXDm99O7dO9/6qampMnLkSKlevboetdakSROZMmVKCbwTlJSsnFx5ePpaScvMkQ614mTEpfWMbhIAeBWvDrq7dOki8+fPl+3bt+vb69atk3/++Uf69etndNMAAPA7M2bMkFGjRsm4ceNk9erV0rJlS+nTp48cO3bM7voLFy6UoUOHyoIFC2TJkiWSkJCgT44fOnTIso7a3uzZs+XLL7+ULVu2yCOPPKKD8J9//rkE3xk86d35O2TtgfPlwd4a0koPLwcA+EjQ/dRTT8mQIUOkUaNGEhYWJq1bt9Y765tvvtnhczIyMiQlJcVm8SRKhwEA/MWbb74pd999t57KZe6RjoqKkmnTptld/6uvvpIRI0ZIq1at9L76o48+ktzcXH3C3HrU2vDhw6Vnz566B/2ee+7RwbyRo9bgPsv3nJJJC3bq6y9f21yqlS1ldJMAwOt4ddD97bff6h36119/rc+4q7ndr7/+ur50ZMKECXrOt3lRZ90BAEDBMjMzZdWqVXqIuFlwcLC+rXqxnZGeni5ZWVkSFxdnM2pN9Wqr3m+VCFX1iqsRbKpH3FtOoKPo5cEenXG+PNj1barL1ZQHAwDfC7qfeOIJS2938+bN5dZbb5VHH31UB9aOjB49WpKTky3LgQMHSrTNAAD4ohMnTuiqIZUrV7a5X90+evSoU9t48sknJT4+3iZwf++993SvuZrTHR4eLn379tXzxi+55BKH2+EEuvdTJ1CemblB57qpWT5Knh/Q1OgmAYDX8urs5eqMuTrLbi0kJEQPXXMkIiJCLwAAoOS88sorMn36dD3PWyVhsw66ly5dqnu7a9asKX///bc88MAD+YLzvCfQ1VxwM9XTTeDtXX5YfUh+XX9Ez99+ezDlwQCgIF79F/Lqq6+Wl156SWrUqKFLhq1Zs0bPN7vjjjuMbhoAAH6lQoUK+sR2YmKizf3qdpUqVQp8rpr6pYLuefPmSYsWLSz3nz17Vp5++mmZOXOmrkSiqMfXrl2rn+Mo6OYEunfbe+JiebBRlzegPBgA+PLwcnV2XJUJU0laGjduLI8//rjce++9Mn78ePEW5FEDAPgDNfS7bdu2NknQzEnROnfu7PB5EydO1PtllaG8Xbt2No+p+d1qcXXUGrx7WPmzP22U9Mwc6Vg7Tu7rUdfoJgGA1/Pqnu7o6Gh5++239QIAADxLDelWmcZV8NyhQwe9/01LS9PZzJVhw4ZJtWrVLLlVXn31VRk7dqxOeKoyk5vnfpcpU0YvMTEx0qNHD52jRdXoVsPLFy1aJJ9//rkeuQbf8/eOE/LPzhMSHhIsrw1qSXkwAPD1oBsAAJScwYMHy/Hjx3UgrQJoVQpM9WCbk6vt37/fptd68uTJOuu5GpVmTdX5fu655/R1Nc9bzdFW5T5PnTqlA281dey+++4r4XeH4srJNcmE37fo68M615Qa5aOMbhIA+ASCbgAAYDFy5Ei92KOSpFnbu3dvodtT88E/+eQTt7UPxvlx9UHZevSMxESGysjL6hndHADwGV49pxsAAADGO5uZI2/M3a6vq4C7bFS40U0CAJ9B0O2GhCIAAAD+bNq/e+RoyjmpVraUDOtcy+jmAIBPIegGAACAQydTM2Tywl36+hN9GkpkWIjRTQIAn0LQ7aKocHY0AAAgcLz3105JzciWpvExck3LeKObAwA+h6DbRcFBlMYAAACBYc+JNPly6T59/ekrG0swJcIAwGUE3S7Ku6thRjcAAPBXr83ZKtm5JunZsKJ0rVfB6OYAgE8i6HYRHd0AACAQrN5/Wn7fcFRU5/ZT/RoZ3RwA8FkE3S4KYVgVAAAIgOosL/+2RV8f1La6NKoSY3STAMBnEXS7KDuHAeUAAMC/zd2cKCv3nZbIsGAZdXlDo5sDAD6NoNtFZzKyjW4CAACAx2Tl5Mqrf2zV1+/qVkeqxEYa3SQA8GkE3QAAALCYvuKA7D6RJuVLh8u9PeoY3RwA8HkE3QAAANBUPe535m3X1x/uXV+iI8OMbhIA+DyCbg9kL09KzyyJpgAAALjVh4t2yYnUTKldobQM7VDD6OYAgF8g6HZRYTH3B4t2SasX/pQvluwtoRYBAAAUX2LKOZm6eI++/n99GkpYCIeJAOAO/DV1UVAhXd0TLiQeGTNrUwm1CAAAoPje+nO7nM3KkTY1ykrfZlWMbg4A+A2CbhflDbnX7E+S/3adMKg1AAAAxbc98Yx8u/KAvv5M/8aFdjIAAJxH0O0ie/ugm6YuM6IpAAAAbvHKH1sl1yTSt2kVaVszzujmAIBfIeh2UVChs7oBAAB8hxqx99fWYxIaHCT/17eh0c0BAL9D0O2qAmLuzOzckmwJAABAseTmmmTC7+fz0dzUsYbUqVjG6CYBgN8h6HYTk8kkXV6Zb3QzAAAAnPbL+sOy4VCylIkIlYd61Te6OQDglwi6XRRcQE+3qmsJAADgCzKyc+S1Odv09ft61JEKZSKMbhIA+CWCbhfVqcCwKwAA4Pu+WLJPDp4+K5VjIuTObnWMbg4A+C2CbhclxJWye7/JVOJNAQBAatWqJS+88ILs37/f6KbAhySnZ8l7f+3U10dd3kBKhYcY3SQA8FsE3S4iezkAwJs88sgj8uOPP0qdOnXk8ssvl+nTp0tGRobRzYKXm7RwpySfzZIGlcvIoLYJRjcHAPwaQbcb6nQr9jq6v191ULYdPePpJgEAAjzoXrt2rSxfvlwaN24sDz74oFStWlVGjhwpq1evNrp58EIHTqXLp//u1ddH92ssIQUlrAEAFBtBt5uCbnse/26d9Hn7b082BwAArU2bNvLuu+/K4cOHZdy4cfLRRx9J+/btpVWrVjJt2jRdZQNQ3pi7TTJzcqVL3fLSs2FFo5sDAH4v1OgG+Mvw8iW7Tjp8Tsq5LImJDPNgqwAAgS4rK0tmzpwpn3zyifz555/SqVMnufPOO+XgwYPy9NNPy7x58+Trr782upkw2MZDyfLT2sOWXu4gV3oTAABFQtDton7Nq8hvG47ku/+Wj5c5fM6stYfl1k41PdwyAEAgUkPIVaD9zTffSHBwsAwbNkzeeustadSokWWda6+9Vvd6I7Cp0Q4v/75FXx/YKl6aV481ukkAEBAIul3Uv3lVGSlrjG4GAACaCqZVArXJkyfLwIEDJSws/8iq2rVry5AhQwxpH7zHwu3H5b9dJyU8JFgeu6Kh0c0BgIBB0O0ihmEBALzJ7t27pWbNgkdTlS5dWveGI3Dl5Jrkld+36uu3da0lCXFRRjcJAAIGidRKgKMwfdW+U9LvncWybLfj+eAAABTk2LFjsmxZ/ilO6r6VK1ca0iZ4nx9URZXEMxJbKkwe6FnP6OYAQEAh6DbQoClLZMuRFBn84VKjmwIA8FEPPPCAHDhwIN/9hw4d0o8B6ZnZ8saf2/T1By+rJ7FRJHcFgJJE0G0gqrcAAIpr8+bNulxYXq1bt9aPAdP+2SOJKRlSvVwpubUziV0BoKQRdAMA4MMiIiIkMTEx3/1HjhyR0FBStwS602mZMmXRbn39iT4NJSI0xOgmAUDAIegGAMCHXXHFFTJ69GhJTk623JeUlKRrc6us5ghsHy7eLakZ2dK4aoxc3SLe6OYAQEDiFHgJyJvwfPGO43Lg1FmjmgMA8COvv/66XHLJJTqDuRpSrqxdu1YqV64sX3zxhdHNg4FOpGbIp//u1ddHXd5AgoOpwAIARiDoNsCtHy83ugkAAD9RrVo1Wb9+vXz11Veybt06KVWqlNx+++0ydOhQuzW7ETimLNwlZ7NypGX1WOnduJLRzQGAgEXQXQKCrIqG7TmRZmhbAAD+R9Xhvueee4xuBrxIYso5+WLpPn390csbSFDeYXcAgBJD0F0Cnp65QZ9hrhQTKZe+vtDuOm/P2y6P9G5Q4m0DAPgHlal8//79kpmZaXP/NddcY1ibYJz/LdgpGdm50rZmOenRoKLRzQGAgEbQXUIe/XatfHVXJ4ePvz1vB0E3AMBlu3fvlmuvvVY2bNigezNNF+pRmns2c3JyDG4hStqhpLPyzfLztdsfo5cbAHwze/mBAwfk4MGDltvLly+XRx55RD788EN3ts2vbDt6xugmAAD80MMPPyy1a9eWY8eOSVRUlGzatEn+/vtvadeunSxcaH90Ffzb+3/tlMycXOlUJ0661KtgdHMAIOAVKei+6aabZMGCBfr60aNHdUkSFXg/88wz8sILL7i7jX4jI5veBgCAey1ZskTveytUqCDBwcF66datm0yYMEEeeughl7c3adIkqVWrlkRGRkrHjh31/t2RqVOnSvfu3aVcuXJ66d27d771VS+rveW1114r0vtFwfafTJfvVl7o5b6iodHNAQAUNejeuHGjdOjQQV//9ttvpVmzZvLff//pzKmffvqpu9voNz5ctNvoJgAA/IwaPh4dHa2vq8D78OHD+roqIbZt2zaXtjVjxgwZNWqUjBs3TlavXi0tW7aUPn366F50e1RPusqSrk7Eq+A/ISFB1w0/dOiQZZ0jR47YLNOmTdNB9/XXX1+s9w373v1rh2TnmqR7/QrSvlac0c0BABQ16M7KypKIiAh9fd68eZYkLY0aNdI7VOR3IjVTfl3PZwMAcC914luVClNUz/TEiRPl33//1b3fderUcWlbb775ptx999265FiTJk1kypQpesi6CpTtUSfbR4wYIa1atdLHAB999JHk5ubK/PnzLetUqVLFZpk1a5ZceumlLrcNhdt9PFV+XH1++h+93ADg40F306ZN9Y548eLF8ueff0rfvn31/ersevny5d3dRr+xLZF53QAA93r22Wd1oKuoQHvPnj16yPfvv/8u7777rtPbUVnPV61apYeIm6mh6uq26sV2Rnp6uj4xHxdnv4c1MTFRfvvtN7nzzjudbhec9878HZJrEunVqJK0SihrdHMAAMXJXv7qq6/qTKlqPtbw4cP18DPl559/tgw7BwAAnqeGf5vVq1dPtm7dKqdOndJzrF3JWn3ixAk9VL1y5co296vbapvOePLJJyU+Pt4mcLf22Wef6aHw1113XYHbycjI0ItZSkqKU68fyLYnnpGf1x221OUGAPh40N2zZ0+9c1Y7QbVTN7vnnnv0MDR/N6xzTfl8yT6jmwEACHCqV7lUqVKydu1aPczczFFPsye98sorMn36dD3PWyVhs0cNU7/55psdPm6mksA9//zzHmqpf3p73nZR1eL6Nq0izarFGt0cAEBxh5efPXtWn4E2B9z79u2Tt99+WydsqVSpkvi7Sxv5/3sEAHi/sLAwqVGjhltqcaskbCEhIXoIuDV1W83FLsjrr7+ug+65c+dKixYt7K6jpqSp44S77rqr0LaMHj1akpOTLYsqVQrHNh1Olt83HBU1sIFebgDwk6B7wIAB8vnnn+vrSUlJOnHLG2+8IQMHDpTJkye7u40AAMABVa7z6aef1kPKiyM8PFzatm1rkwTNnBStc+fODp+nEreNHz9eZs+erWuDO/Lxxx/r7ZunpBVEJWuNiYmxWeDYW3/u0JdXtYiXhlXOZ7IHAPh40K3KiKgkLcr333+v53up3m4ViLuStAUAABTP+++/L3///beeS92wYUNp06aNzeIKVS5M1d5Wc6+3bNki999/v6Slpels5sqwYcN0L7R1jpcxY8boYeOqtvfRo0f1kpqaarNdNR3tu+++c6qXG65ZdyBJ5m1JlOAgkUd61ze6OQAAd83pVtlJzTVB1VAylRBFZTjt1KmTDr79nqnkXiojO0fCQ4JdSoYDAAgcapSZuwwePFiOHz8uY8eO1cGzKgWmerDNydX279+v9/dmanSbyno+aNAgm+2oOt/PPfec5baa620ymXRNb7jXm39u15cDW1eTuhXLGN0cAIAdQSa1F3SRmq+lzlarDOYqcYvaIauhZ6rUSP/+/fWO2luos+uxsbF6Tpi7hqct2HpMbv90hbjb9/d1ltCQYImPjZSle07J9qNn5P0FO+W61tXkzcGt3P56AAD/2j/5Kz4r+1btOyXXT14iIcFB8tdjPaRm+dJGNwkAAkqKk/unIvV0qzPgN910kzz66KNy2WWXWeZ6qV7v1q1bF73VAe6PjUfl43/25Lv/xzWHCLoBAICNN+ae7+W+oW11Am4A8GJFCrrVMLJu3brJkSNHbBKi9OrVS/d++zuTh8aXr9hbvCQ4AIDAo4Z7FzQFyR2ZzeF9luw6Kf/tOilhIUEy8rJ6RjcHAODuoFtR5UPUcvDgQX27evXq0qFDh6JuDgAAFMHMmTPz1e5es2aNToZGrWv/pGYGvvnnNn19SPsaUr1clNFNAgC4O+hWJURefPFFXSbMnKFUJVZ77LHHdOkS6yQr/sj1WfAAAHiGKuNpb0Ra06ZNZcaMGXLnnXca0i54zuIdJ2TF3tMSHhosD1xKLzcA+GXQrQJrVW/zlVdeka5du+r7/vnnH52p9Ny5c/LSSy+5u50AAMAFqqLIPffcY3Qz4IFe7jcuZCy/pWNNqRIbaXSTAACeCLrVkLWPPvpIrrnmGpuM5tWqVZMRI0YQdJeQ/SfTZePhZOnXrIqez3c2M0fUtL7IsBCjmwYAMNDZs2fl3Xff1ftl+Je/th7TtblLhYXI/T3rGt0cAICngu5Tp05Jo0aN8t2v7lOP+TtvGV5+yWsL9OX/bm4jlzepLI3HztY1vbeO7yvBwdT1BoBAUK5cOZtEaqon9MyZMxIVFSVffvmloW2De+Xmqrnc53u5h3WpKRWjI4xuEgDAU0G3ylj+/vvv67Po1tR9qscbnpOUnimxpcLkwKmzlvtW7j0tLRPK6uuZOblyNitHSkcUOUceAMCHvPXWWzZBt8qrUrFiRenYsaMOyOE/5mw6KpsOp0jp8BC59xJ6uQHAVxQpMps4caL0799f5s2bZ6nRvWTJEjlw4ID8/vvv7m4jLli+55Tc+MESuapFVfl1/RGbx6z7tb2kIx4AUAJuu+02o5uAEpCTa5K35p3v5b6jW22JKx1udJMAAE4qUprxHj16yPbt23VN7qSkJL1cd911smnTJvniiy+KskmIyPqDyQ4fW7jtmIydtVFfzxtwK9YlWtXQQjW/+7mfN8l/u054prEAAK/wySefyHfffZfvfnWfysEC//Dr+sOyPTFVoiND5a5udYxuDgDABUWu7RUfH68Tpv3www96USXETp8+rbOa+zsjepJv+2SFbD16xu5jKuAOsurrzjWJTFm0Sz79b6/cNHVZCbYSAFDSJkyYIBUqVMh3f6VKleTll182pE1wr+ycXHln3g59/e7udSQ2KszoJgEAXODfBbU9RPUkexubvGkmkQOn0g1sDQCgpOzfv19q166d7/6aNWvqx+D7flp7WHafSJOyUWFye9daRjcHAOAigm5/YRV055pMRcperrKibjyULFk5ue5tGwDAY1SP9vr16/Pdv27dOilfvrwhbYL7qH3yu/PP93Kr5GnRkfRyA4CvIej2A6pXO9i6XIyIhFhP8nbS5EW75Kr3/pFHZqx1av3ZG4/INe//I3tOpLn8WgAA9xg6dKg89NBDsmDBAsnJydHLX3/9JQ8//LAMGTLE6OahmL5fdVD2n0qXCmXCZXiXmkY3BwDg6ezlKllaQVRCNZS8uZsTZcJ1zfP0dLu+nQ8W7dKXv60/IpNuKnz9+75crS+f+G6dfH9/F9dfEABQbOPHj5e9e/dKr169JDT0/G49NzdXhg0bxpxuH5eRnSPvXejlvq9HXYkKpxwoAPgil/56x8bGFvq42sn7O++b0S3y2pxtlusqc7l1z7en31fKuawiPhMAUFzh4eEyY8YMndB07dq1UqpUKWnevLme0w3fNmPFATmcfE4qx0TILZ34PgEgIIJuVZYE3mn6igOW690nLpBqZUtZbr8+Z5vc1rWWVCgT4ZHX9sK8cgAQcOrXr68X+IdzWTny/l879fUHLq0nkWEhRjcJAFBEzOn2U4eSzlquv79gpzz0zZoC109Kz5Qz57JLoGUAAHe6/vrr5dVXX813/8SJE+WGG24wpE0ovi+X7pNjZzIkPjZSBrdPMLo5AIBiIOgOkJ7d/3adlCvfWSwfLd5tcxb9v50ndGbUYdOWG9o+AEDR/P3333LllVfmu79fv376Mfie9MxsmXIhz8qDvepLRCi93ADgy8jIEUA2H0mRzb+lyOVNKkt4aLA89/MmmbMpUe7sVlvWH0wu8nZ98BwEAPiN1NRUPa87r7CwMElJSTGkTSiez/7bJydSM6VGXJQMalvd6OYAAIqJnu4ADDN7vLZQOk/4Swfcyqf/7XW4rslkkq1HUySb2t0A4JVU0jSVSC2v6dOnS5MmTQxpE4ouNSNbPvj7fC/3Q73qS1gIh2oA4Ovo6YYuMVZQ7e6Js7fJdW2qyZs3tirRdgEACjdmzBhd0nPXrl1y2WWX6fvmz58vX3/9tXz//fdGNw8u+mXdYUlKz5Ja5aNkYKt4o5sDAHADrz99eujQIbnlllukfPnyljIoK1euNLpZfsVezP3frhP60pw59cfVh2TnsVT538KduiSZ7fN9u+cfAHzZ1VdfLT/99JPs3LlTRowYIY899pjed/71119Sr149o5sHF/2w6qC+HNKhhoTSyw0AfsGr/5qfPn1aunbtquel/fHHH7J582Z54403pFy5coa2yzrGbFQlWvzRTVOX6Uvrat+931yke73fmHuxJriiPo7X5myVyQt36aRsb8/bLqv2ndbB+Mw1B2XTYefni6vn3P35Snnqh/Vuey8A4O/69+8v//77r6Slpcnu3bvlxhtvlMcff1xatmxpdNPggj0n0mTlvtMSHCRybetqRjcHABAIw8tVCZSEhASb+uC1a9cWbzJrZFf5duVBGfPTRgkUq/eftrm9+3iaTFpwfv5ZWEiQvD1vh14+u6ODPDpjnb5/7yv9ndr2ruOp8ufm83PNX7m+hdvbDgD+SmUq//jjj+WHH36Q+Ph4PeR80qRJRjcLLvhx9fle7u71K0rlmEijmwMACISe7p9//lnatWun64xWqlRJWrduLVOnTi3wORkZGTpbq/XibtaDqVUZj4RypcRfBQVZ93U7FzSbbT3i+mefnctQdQBw1tGjR+WVV16R+vXr631lTEyM3g+q4ebq/vbt2xvdRDgpN9ekp3Ip15OxHAD8ilcH3WqI3OTJk/XBxJw5c+T++++Xhx56SD777DOHz5kwYYLExsZaFtVT7mn+HCYGeeD9qkzoO4+dsTsXPMjqFZkrDgAFz+Vu2LChrF+/Xt5++205fPiwvPfee0Y3C0W0dPdJOZR0VqIjQ+WKJpWNbg4AIFCGl+fm5uqe7pdfflnfVj3dGzdulClTpsjw4cPtPmf06NEyatQoy23V0+3xwNtPY0MV9J7JyLZzf0HPcbwtc6/5fV+ulnlbEqV348ry0fB2BW7LxY52AAgYKteJOhGtTkirk9Pwbd9fGFp+VYt4iQwLMbo5AIBA6emuWrVqvhqjjRs3lv379zt8TkREhB5eZ714mslPo+7xv25x+NjxMxkuBd1fL7/4namA2/rSmnWQ7Z+fKgC4xz///CNnzpyRtm3bSseOHeX999+XEyfOV56A79Xm/mPDUX19UFsSqAGAv/HqoFtlLt+2zTZT9vbt26VmzZpipLyBpb+Ogp727x6Hdb3bvzTPpW19tNj+tvIOIbfu2GZ4OQA41qlTJ53n5MiRI3LvvffK9OnTdQI1NUrszz//1AE5fMMfG47I2awcqV2htLSpYWyFFgBAgAXdjz76qCxdulQPL1f1R7/++mv58MMP5YEHHhBvUlBsOO02x8OnfVVmdq5T61n3WjsaJv7E9+dLg+XkmuTAqXS3tA8AAknp0qXljjvu0D3fGzZs0HW6VRI1lYD0mmuuMbp5cML3F2pzX9+mmssJTAEA3s+rg26VdXXmzJnyzTffSLNmzWT8+PE6WczNN98s3qSg/tjLGvlfMpQC53Q7+DSCHRxEmA80Rn69WrpPXCA/rztstS0AgCtUYrWJEyfKwYMH9b4T3k+dcF6255Q+OX1tG7KWA4A/8upEaspVV12lF2+SN7AMtGHQOQW83xV7bWt4mxV23v6Pjefnsn2waLflvgD7WAHAbUJCQmTgwIF6gXczlwnrUre8VCvrvyVIASCQeXVPt69IiIuSQFLQSYY9J9Ls3r/bwf352CRSI+oGAPj3/vSHC1nLr6eXGwD8FkF3EeSNORtXjZF3hrSSQOFsKJxjNfVbzdl2+XWIuQEAfkyNDtt/Kl1Kh4dI32ZVjG4OAMBDCLrdZEArxyU+wkP962Pefdy5XutXZ2/1WJI2f7HpcLLc8tEyWXcgyeimAABK2A8X8ppc2byqRIV7/Yw/AEAR+Vc06KXKRYUZ3QSvcCzlnN37F2w7FrA93TdNXSb/7DwhA//3r9FNAQCUoLOZOfLbhiP6+qC2DC0HAH9G0F0ErsaCgRA8OqPDy/Pt3n/7JyuKtL2pf++W9+bvcPl5J1IzZMxPG3Uvs9GSz2bpS34jABBY5mw6KqkZ2ZIQV0ra14ozujkAAA9iLJMHPdu/sb7MJaIqkryJ1NS88OAgVfM7SA9Df+n3Lfr+we0TpFJMpA7CoyNDZUiHGgVut92L8/TlF0v3yd5X+nvwHQAAIAWWzLyudXUJVjs3AIDfIuj2kId71Ze7utfR14uQQwx5en/PZeXIpa8vlIZVouXT2zvYnMjIyM6VQ0lnbYJwFZjbk5Gd4/mGAwBQgMNJZ+XfXSf0dbKWA4D/I+guAmfqcgdbBX30dBeN9ae2ZNdJOZJ8Ti/51jOJpGVk29x2EHMzjBsAYLiZaw7p/VGH2nFSo3xglR0FgEDEnG4PsR4plktXt02PtbOe+G6dPP/LJt07rUqqOD0M/UJkfTI1Q654a5FMWbRL3OHMuSyZvyUx4DKsAwDcXJv7wtByEqgBQGAg6PYQ6/lZ9K5edPV7/zi97h8bj8on/+6Vhs/OlnE/b7Lcv2rfqXzrrtl/Ot/IgkkLdsn2xFR55Y+tDr+LuZuOOt2eOz5dIXd+tlLe/HO7088BAF8zadIkqVWrlkRGRkrHjh1l+fLlDtedOnWqdO/eXcqVK6eX3r17211/y5Ytcs0110hsbKyULl1a2rdvL/v375dAtOZAkuw+kSalwkJ0qTAAgP8j6PaQ6uVKWa4zvPyiHcdSi72NlXsvBtjWAbrZidRMh/O38/aK3/PFqnzJ2hxZceF1v191oAitBgDvN2PGDBk1apSMGzdOVq9eLS1btpQ+ffrIsWP2SzsuXLhQhg4dKgsWLJAlS5ZIQkKCXHHFFXLo0CHLOrt27ZJu3bpJo0aN9Prr16+XMWPG6KA+kBOo9W1WRcpEMMsPAAIBQbeHXN0i3nK9RvnShrbFH+U9j7F4x/mENEqPiQvOr2PneQWN9P9o8W5pOm62rD2Q5FQblu4+KX9uTpRxszbKhAtJ3ADAl7355pty9913y+233y5NmjSRKVOmSFRUlEybNs3u+l999ZWMGDFCWrVqpYPqjz76SHJzc2X+/IslIp955hm58sorZeLEidK6dWupW7eu7vWuVKmSBOIUq1/XHdbXGVoOAIGDoNsDOtWJsxlePuWWNoa2x9/d/ukKmx7q7AvX7Q0wKGjUwYu/bZFzWblyX57e77zMmxjy4VK5+/OV8tmSffLB37tdmq8OAN4mMzNTVq1apYeImwUHB+vbqhfbGenp6ZKVlSVxcefrTqsA/LfffpMGDRroHnMVaKsh6z/99FOB28nIyJCUlBSbxR/M25IoKeeyJT42UjrXKW90cwAAJYSguwgKGy0eJLaps2vS0+1WpjzDxHcfTytgzTz3OJED7WhK/gzpeaWcy8p339ajZ/Tl6bRMpzLcu4NKFkewD8AdTpw4ITk5OVK5cmWb+9Xto0edy3/x5JNPSnx8vCVwV8PSU1NT5ZVXXpG+ffvK3Llz5dprr5XrrrtOFi1a5HA7EyZM0PO/zYsatu5XtbnbUJsbAAIJQXcR5J0XjJJ3OKnwwDhv3KuC05V2krA5Cmat7TtpG9g/Mn1tvud8/M8enZit9fg/bRK/eUpiyjlp++I86frKXzb3Z+fkyqLtx+2eGAAAT1GB9fTp02XmzJmW+dqqp1sZMGCAPProo3oY+lNPPSVXXXWVHrruyOjRoyU5OdmyHDjg+7k0jqWck7+3H9fXr2tTzejmAABKEEE3fNJbTmQQzxt0q6HgKvu4M1Qw+/mSvZbbPV5baPP4X1vzJxVSvdvmTOmfL9knnqZqlysn084njjNTQ92HT1sut3y0zONtAOA/KlSoICEhIZKYmGhzv7pdpUqVAp/7+uuv66Bb9WS3aNHCZpuhoaF6fri1xo0bF5i9PCIiQmJiYmwWX/fT2kM6r0ibGmWlTsUyRjcHAFCCCLrhk7Iv9J44kpWTm29EgnWyNWeMnWW/tzpvkGtm8pJM9eb6r+sPJhvdFAA+JDw8XNq2bWuTBM2cFK1z584On6cSpI0fP15mz54t7dq1y7dNVR5s27ZtNvdv375datasKYFCnZQ1Dy0f1NY/hsoDAJxH0O2lpt1me+ACWzmFzM3+7L+9Nj3dKggviubj5sjhpLPOrWyynzG9xDFNEEARqXJhqvb2Z599pmtr33///ZKWlqazmSvDhg3TQ7/NXn31VV3+S2U3V7W91dxvtah53GZPPPGELkWmtrtz5055//335ZdfftFZzwPFxkMpsj0xVcJDg6V/C2pzA0CgoUBkEZREZ+ZljWwT2cBWYYnKlu4+pbPEmr0+17aXxdrkhbvk/p517T52JiNbXpvj+Ll5FVTn29qXS/fJtH/3iKepWuURoSEefx0A/mHw4MFy/PhxGTt2rA6e1Rxs1YNtTq6mhoSrjOZmkydP1lnPBw0aZLMdVef7ueee09dV4jQ1f1slR3vooYekYcOG8sMPP+ja3YHi+1Xn56T3aVpFYkuFGd0cAEAJI+gugtAQBggYzVwWzJFVeRKmfbBot8N1X5291WHQ7UogrYazOzoX8O78HaIS1Y68rL6+/exPG8VZ6r2kZeTIJQ0qnn8dk0kysnNl94m0Qju6Gz47WxY+3lNqVSCDPgDnjBw5Ui/2LFxom99i796LuS8Kcscdd+glEGVm58rPF2pzX08CNQAISATdRdCnaWVpW7OctKtZzunn1K5QWvY4CJLyKhVGz2RhCps7fTrdfZm7T6bZZjJ3RDUp084wdlVC7M0Lid+GdaklMZGu9XJcP/l8fdzlT/eSSjGRcvNHy+S/C0nU7AkKCsqXVX38wGYuvSYAwD1U4k21T6oUHSHd658/eQoACCwE3UWghuv+cH8Xh4/niXngZsdSMoo8R7so/t3pOMC1tnr/aTl+Jn+Abt3W7BzHJwtW7TutT+Y4cuxMhg66Cwq4lbw/P0rcAYBxzAnUrm1TTUKozQ0AAYmgGz6nJOZCF0ViioMecSePsVbuPVVg0A0A8C0nUjNk4bbzJSYHtaludHMAAAZhcrIXomey5O08dsbt21y847jLCeCKKjsnV+ZsOionUzNKdKTF3E1HZffxi1mKAQAXzVp7WOcgaVk9VupXjja6OQAAg9DT7YW8oNRzwHnxty1u3+atHy+XpaN7FWs4oUrA44ypi/fohHBKeAkl+lMnFe75YpW+vuOlfrLuQJK0TCgrYSQaBADthwtDy69vSy83AAQyjo49oE2N4g0Rji9bym1tgbHylhszudjbfcMH55OoFWb2xiOW6/aSuRVXrp0M7mv2J1muPztzowyaskRe+GWz218bAHzR5sMpsvlIioSFBMnVLeKNbg4AwEAE3W40b1QPeebKxjLysnpF3ka3ehVk6rB2bm0XjPPD6oP5hnu/5KBX3d6wcNV77JQCxpQXd+REWka2dH31L3l4+hqHGeRnrDxfg/aLpfuK92IA4Ed//5XejStLudLhRjcHAGAggm43qlepjNx9SR2JLEbJr8/v6KC3g5K1cFv++dfuMmPF+YBUUXHqR//YTwT38u9b5Y252yy9ymfOOV/2zJMJcf/YeFSOJJ/TcxOtOVm+HAACjqpaMWvtIX39ehKoAUDAI+j2MpQb8+8h5oUlyXvvr50ya935A7Wvlu3P97ijoelG/Gw8lRQOAHzd39uPy4nUTKlQJlx6NKQ2NwAEOoJuoAQ9/t36Qtc5eOqsvsyyk0TtwW9sh3ibBRdwtubr5fsdBsh571e3887fdrRlYm4AKLg294BW1UguCQAg6PYGd3Stbbke5Iau7nt71LG53aAyw9W9qfejMOafgL2Y9tf1Rwp8jqPgWA0Rz2v+lkRpPf5P+WtrouW++79cLXWe/l2emblBElPO6VJk6ZnZdrdrPacbAHDe6bRMmb/lfG1uhpYDABSCbi9weZPKbt3eI70aWK5HR4RKjbgot24fnrVy32l55Y+tsvOY8/WvCztZs+VISr777vxspSSlZ8kdn6603Dd701HL0HbVq37F23/LmFmb7G7T0Zxu1VM+fNpy+b/v1zndfgDwF7+sP6yrSDSpGiNN4mOMbg4AwAtQp9sPMS/c95O6uZrYrbCvPKcIWc/WH0ySc1mOy485mp++5WiKLLrQoz9xUEuXXxcAfBm1uQEAedHT7QOGdkgoetBNAI4iBt0hhZy9cTS6PNf9ZcIBwCfsSDwj6w4mS2hwkAxoRW1uAMB59HR7sT8fvUQPMVbD1L5ZfrHsVGGCrCNtpt0GhGV7ThX4eHYRgu60zJwCH8+bcM2eA6fSJcHN0xte+GWzJJ45J+8Pbe2WHAgA4C7fX6jN3bNhJalQJsLo5gAAvAQ93V7A0TDd+pWjpV/zqi5vL28cMvrKxhIdGSqjLm8gjapEF7WZ8GGeSHrmKOa2/v31e2exvkzNyJZ7Pl8pv6yzrfVdFNP+3SO/rT8iW46cKfa2AMCdI4p+WnO+5OOgttWMbg4AwIsQdJcQ6zh49iPd5QbruV6FxEPqjLkaquasvOWj6lYsI+vGXiEP9aov0+/p5PR24D+Bs83oh2LW2M7IztEHl860RwXbyv8W7JS5mxMdljwriqwcxrED8B6LdxyXxJQMKRcVJpc1cm+CVACAbyPoLiHW4UmjKjES6kLdzthSYbLphT4y8tJ6Tq0fZOd1gy8E7WWjwp1+XXiXt/7cXuTnWp+H0VnJ3/q7SNs5l5UjLZ6bKy2em+NS4H4iNUPcbcexVOk8Yb58vWy/27cNAK76YfX5Xu5rWsZLeCiHVwCAi9gr+IiI0JACs5J3rlNeX3aqE0f2cj+1wMWM5tY+/mePJJ/N0j3Uaoi3CljzOn6m8MB40+FkycjO1fO9P1uyr9hD0VXgXtRed1WS7EjyOXl65gZ9W+U/eGPuNv0+i5L8aPSPG+Rw0tkitQVAYFN/d+ZeKLk4qK1ryU8BAP6PRGolpKA42B2zbf93cxtdG/TqFvEkl4JdLZ+fK61rlLX72Om0TLnj0xVObKXw39b2xPxzre3VHFfB9o0fLJHMHJPMvL+LZTRGUQP5y99apDOqHzx9Vt4a3MqlbV39/j+6PJqqZ/7TA11dei4AqDwT6oRkg8plpFk1anMDAGzR022QWuVdz+h8a+eaEh1h/zxJudLhMqxzLX1pzV4YUzkmQsJCCMwD0Zr9SXbvbz3+T9lwKLnQ5ztzPucrO8O9U87l731OOZctK/aelnUHkuSYE73shTF3mK89YP89FsRcj3zz4ZRitwNA4PnhQtby69tU58Q3ACAfgm6D3Na1ltxzSR355m7nE5tVio6UteOucOl16lcuk+++f568TDY818el7QBFlZ6ZLbuPpxW4jjuPUQvb1JifNsrwacvtljxzVEkAABzZcyJNVu07LWqwzrWtyVoOAMiPoLuEmOzM0X76ysbSue75udjOCnFxCG7eTOZKWEiwRIaFuLQdQHFm+nXeX9yQD5fm2caFjbgxvt17wiqoD7qYNd1cK3z2xqP6dV/8dbN8sXSfLNp+XNYedL1H3DqDe3EywAPwH79eKIV4SYOKUikm0ujmAAC8EEG3F/DksTuj3OBOj3271uXf3PqDtsPWx87aJPd9sUrmbD568TlWj6tkb/YkpztOkHbDB0ss11WverNxc+TNudv07e4TF8h9X66SOZuOykf/7LGsZ6+nOyvHJH9sOFLQ29PtaDp2Tr6TCQAC0/oLU3MuqV/R6KYAALwUQXcJaVg52m3b6tWokscCenNHergLJc0QOPaeTC/2NlRP8+xNR+X/vl+ff/sn0qTV83N1FnJrkxfukpYvzJWvltnPmG4v8/q7f+20ub18z2mb2+b/Gh8t3m1z//1frZZlu0/KF0v2yg1T/suXDX3+1kTJzjXJsj2nCnmnAAKBORdEk3gSqAEA7COyKiEvXdtMbu5YQ34Z2a3Y23prSCuZeH0LnbHc3eY/1lOubhkvs0aSwRlFE+REhnM7T9Jem7tNzmRky3t5AuZXZ2/Vl8/M3OjSZudvSbz4Eg6a9eJvW/Ldt/lIioyZtUknevtg0S6XXhNA4FAjXw5dKDXYuCpBNwDAPkqGlZDyZSLkpWub233M1eRNMZFhcmP7BNl1PH8ZpuIOL69dobS8N7S1a08CPGTfyTSJL1uqyM+/87OVxW5DemZOsbcBwD9tOnJ+aHlCXCmJLRVmdHMAAF6Knm4vYC/ZmTPqViwj7w5tXWAG9IKGl88b1UMGta0uJWVI+4QSey34no2HkmWD1fxv1Uvd47WFcstHy9yyfXelNyBPAoB8Q8vp5QYAFICg2wt0qB0nLarHynVtXC81ck3LeJczoJvVq1RGXr+hpbw2qIUUx3f3dXZqPZU1HQEgqGgB91Xv/SP7T6Xn66V219zpvMFyQSekCnqsSMPn3WD38VQZ/MESWbzjuCGvD0DsTkVRmlSNNbopAAAvxvByL6CC0Z/dMNe7qL1yxUmeruZ/t61Rzql1qYEMR/7aeszjrxHkQhe1N/5SH5q+RjYeSpFlHy+Xva/0N7o5AEiiBgBwEkE39DzuoljweE+pERfl9PqUNQ4My4vQM/3l0v3iTYpTg1uVIgsODtJZz1U970rR7qnbay9DOwDjqP/fO4+dz63SlKAbAFAAxvtC2teK08PMfxzRxeb+569panM770GFCtZDgoOcnuNKzA0jZWbn2tz+38KdcuOUi/W9rS3afnEId97fd0G/d1XSrNULc2XdgSRp+fxc6fDS/HwlxwD4hx2Jqbp8YNmoMKka656TawAA/0TQDU0lVGuTZ5j48C615I+Hu1tuqyHwKnFbUdHTDSN9+t9em9sLtx2X5Xvt98ov3nGiSK+hSpqlnMuWAZP+tdxn7gkrTm+akXPJARSeRM2V6SsAgMBD0A2nA2V1SFG8wwqibviXX9cf1pdpGdny5p/b3f67/2XdYWn47Gz5Zvl+sqYDXptEjaHlAICCEXT7OXf2LquD/uIc+PdsWMl9jQG8wMiv1+jL1+Zsk3fn73D79h/85vz2R/+4wekTXjm5JjlzjiHtgKdtOny+xGHTagTdAICCEXSj2JwdVlclhjlv8D3nsnLk53WHC5ybveHQxfriRk+ruPGDJdL8ublyKOmsTXK7QZP/swQJAIpHJUzccuSMvk65MABAYQi6/Vxxh6Ral/lSwXWZiIIT3k+/p1PxXhDwMt8sPyAPfbNG7v3ifN1wo5xMy7Q5EeDIqn2n9eWv684PfTcH4iv3nZZhHy/3cCuBwHDgdLqkZmRLeGiw1KlYtAogAIDAQckwP+fuXrZL6leU69pUcziHrVRYiHtfEPASS3efsjuyY8muk0XeZnZOrrw+d7s0rxYr/VtULXDdDKvs62oIeXEDdwDFT6LWsHK0hIXQfwEAKBhBN1wK2lX94TdvbGVUcwCvq+E9dOpSaVOjrMP1s3IcB8gqQdqURbv09ZYJl0r1cs7VvSclIWCsTReCbupzAwCcwelZP0fGY8B9Zq09JA9PX+vSc1RQ7siu42mW68su9KQ78385b+C//+T5oa4ASjhzOUE3AMAJBN0oUN2KZfxm2HhIsHNnIMqXDvd4W+CbHAXc7uh5fuy7dTo5kyPBVlG39VqqDvglry2Qji/Ns1nfvPrS3UUf/u6M9EzPBvvqBMPH/+yRf3cWrXY64Oka3QAAFIagGwUqFR4iG5/vI2vGXi7eJCGulMvP6dGgolPrZRdxvixQ3JEoiwsILK1XzbKa3714x3F9mZZpP7na1L93i6e88MtmaTJ2jqzcW3AvfXH8u/OkjP91s9z80TKPvQZsTZo0SWrVqiWRkZHSsWNHWb7ccQK+qVOnSvfu3aVcuXJ66d27d771b7vtNp0LwXrp27ev+KqTqRlyNOWc/v/biKAbAOAEgm4USmUsj3Syp7ukhrPfc0ldm9v9m1fVJcnev6l1sbY7b1SPAnsbgaJStbMHf7BEPl+y13JfUJ7q27uPp8qBU+mWJGuO/m8N/vDikHXrkeY9X1vgcNtmKgv7/C2J0uGlebJo+/mAvaim/btHX06cs008mSUaJWfGjBkyatQoGTdunKxevVpatmwpffr0kWPHjtldf+HChTJ06FBZsGCBLFmyRBISEuSKK66QQ4cO2aynguwjR45Ylm+++UZ8fWh5rfKlC63oAQCAQtANj1EHIxWjIzyz8TxzWlvXKCtLn+4lV7WIL+AphQfT9SqVkV6NK7mliQgcWy4chBfko8V7ZNmeUzJ21iaHJ6me/2WzdJ+4QBZuO6aHVFuzDqLVkPLElHP6uvWveu/JwgPUOZsS5c7PVsqxMxkyfJp7Sohxosp/vPnmm3L33XfL7bffLk2aNJEpU6ZIVFSUTJs2ze76X331lYwYMUJatWoljRo1ko8++khyc3Nl/vz5NutFRERIlSpVLIvqFfdVDC0HALiKoBtuN7RDDWlUJVpWjekt7Ws5f2A1ul8jfXl5k8oFrrfimd4FPv7jiC5SHOMHNpOqsZHF2gYCy7ks215pZ+c+OxoY8uXSfTJvS6LNfZl5er7NidMKO5lUEuFwjrtrE1ohF2TJyczMlFWrVukh4mbBwcH6turFdkZ6erpkZWVJXFxcvh7xSpUqScOGDeX++++Xkyc9m2vAk0iiBgBwFeOi4FaqN27Cdc0tt105Fr+3R10Z3qWWHsredOxsu3NU/33qMt17nnez1rWT29Qo51SPtuottCc6Mkxu7VxTJs723JBZBBY1pNw6EdrB0+n6d+ZoOoYr/2+KWrM7LzWcPbSI9Ybp6fYPJ06ckJycHKlc2fbEp7q9detWp7bx5JNPSnx8vE3groaWX3fddVK7dm3ZtWuXPP3009KvXz8dyIeE2J+6lJGRoRezlJTCR5OUFHq6AQCuoqcbblU6onhZzs1zxx1lGq9W1n4CtcJ6+xb/36U2t0OCgmTi9S2K3E7AFWpIufWJoW6vLpCWz8+Vr5btt7u+MyFsRlauDpQn/OE4GDqafE7+2mp/Lq61XcdTpem4OfLq7K26R/61OVtlw8Fk8XRP94Ktx+S2T5ZbhsrDt73yyisyffp0mTlzpk7CZjZkyBC55pprpHnz5jJw4ED59ddfZcWKFbr325EJEyZIbGysZVFzxb3B2cwc/f9FoUY3AMBZBN1+zoOjPvMNyX60dwOpc6HEWEm/fkHUUPeEuCibQKZWhSi5oV11A1uFQGPvPFK6g4zj6iTSir2nC+09338h6ZojnSbYzqu1p+srf8kdn66QjOxcmbxwl7wzf4dMWrBLrn7/nwKfl2U13H3joRQ5lZYprrr90xWycNtxGfPTRsOTMzpDJaCbOHur3/bsV6hQQfc8JybaTm1Qt9U87IK8/vrrOuieO3eutGhR8AnNOnXq6NfauXOnw3VGjx4tycnJluXAgQPiDbYlnhH19VcoE+65nCUAAL9D0A23uLVTTXm4d/1895vcMKM0LCT/UXfL6mVtblv3IjqjXFS4fk7ZqLBitw9whis/UWfK1p1My7QZsp7X7E1HnXqtQ0lnZZ9VAratR87kW+edeTvk6vf+kbQL88iV+s/8YbPOLcUo6XU89eIw4qLacyJNrnpvsfyx4Yh4ikpA97+Fu+Q3D76GkcLDw6Vt27Y2SdDMSdE6d+7s8HkTJ06U8ePHy+zZs6Vdu3aFvs7Bgwf1nO6qVas6XEclXouJibFZvGloeeOqMS7vdwAAgYug28/56zFBy4Sy8vVdHV16jiu97o7KLblL5Rh6SAKNdWBbmMU7HNfrtlZQ0L1qX8E95a54a9522XAoWaYuPl/z+7SdXm1zcimjPPbtWt3jfv9Xqz3+WkeSz4q/UuXCVO3tzz77TLZs2aKTnqWlpels5sqwYcN0L7TZq6++KmPGjNHZzVVt76NHj+olNfX8EGx1+cQTT8jSpUtl7969OoAfMGCA1KtXT5ci8zWbj5yfdkESNQCAKwi64VHuGF5+c8ea+rJ7/Qo293epZ3u7MHGlw8WbRgYgsPzugd7Rkj6p9va8HfLfzhMlkhHd1RNgq/cnebwtgWDw4MF6qPjYsWN1GbC1a9fqHmxzcrX9+/frOttmkydP1lnPBw0apHuuzYvahqKGq69fv17P6W7QoIHceeedujd98eLFujfb12y60NPdND7W6KYAAHwI2cv9VEJcKTlw6qxc2dzx8L2S4I6D86evbCw9GlaUDrVsS9BYC3JyOzPXHBJvoU4CFGUeLHyTu6cBqxNaqu63uyWlX/xNPjpjrdSvbJun4fW52+T/+p4v71ccGw8lO0xUdevHy6Rnw4oy8rL8U1bc7cy5LH1C5IomVaScF52YM9LIkSP1Yk/e5Geq97ogpUqVkjlz5og/UJUCzNMvyFwOAHAFQbef+mVkN1l7IEm616/otT3d6qBaJVGyx3quXHhosFzasFKBr+Mg2bkNV5LeuGMuemFqlY+yBN0jetbVc0UBZ63efzpfLW93WGeVtdzeSSrVozzkw6VumYNt7+/Ed6sOyMp9p/Xi6aB7/8l0efn3LXr++7crD8oP93dx6nnekCASJW/vyTQ5m5UjkWHBUrtCaaObAwDwIQwv91Nlo8KlZ8NKDktvlRzHR6fvDG5d7GGz91xSR+pXKiM3tHOtnIy9+XhVYi6WuHFWgzy9gEX1UC/P9+jBv/jyKInDSWflxd82OyyFZsNDf8K+X3VQLnltgSXhnCtz4Im5A5M5iVqjKjFesG8FAPgSgm4Y4t4edSQ2KkyubF7FkhitKNSQ8T9H9ZDSEY4HbbSrVc5y/beHusmz/RvLTR1q5FuvermLNcAHtbEtJ3Zdm2p6yWvSTW3EHcz1yYFAcNdnKyUxJcPuSTZnMrcX5tsVB+TPzQWPAvjfAsflqgB7zMkCSaIGAPDroFvVAFXDjh955BGjmwInORqG+djlDfXlq9e3kAnXNZdPbmvvsTaM7tfYcl0lv7mrex0JDcn/07duaqWYSNn2Yl9ZM+Zyef2GlvLiwGZyVYuqdkcUFJU7e8va1Cgrm573vUzACAwPfL1a7vpsha4/7ijT+bmsHD1k/tXZW4v9ev/3w3q5+/OVBa5DbzWKnkSNoBsA4KdzulesWCEffPCBtGjRwuimwAWODmzVPG0lOjJMhtrpdXaXStERBfaCFyQiNEQvg9ra9npbK+8g8VKFMuHy8rXN9TDa536xP4zW3Yr6PgF3yczOtfzfNkvNyJbf1p/Pdn04+ZxUK3txRInZ+oPJ0mjMbLvb9MZBvMzpDuzh5SRRAwD4ZU+3qvN5880369qh5cpdHCoM/2XEgbb1MHRnBTuY13d1y3i5omkVua1rbd2Tr4auX9My3g2tBLzXD6sP5rsv1ypCXX8gSScus5cV2hHrpIrurD1u7nUHnHHszDk5kZqhk3aqOd0AAPhd0P3AAw9I//79pXfv3kY3BcU4sDWiJ7aww2rr4+5Hezdw27zrmMgwy3XVk//Pk5fJu0NtE8cN71xLX3Zzst54u5qccIJ3m1jI0PD7v1otH/69u8jbv37yfy6VHwPc3cutspaXCicHBwDANV4/HnX69OmyevVqPbzcGRkZGXoxS0nJP3cQxgS99SqVkZGX1tNDr71RYUF1p9rlZWCrePlr6zFJOZddYCm0uy+pU+jrDWxdTZpVi5EacflLz3x4a1u554tVNvd9fFt7afn8XPEHrRLK6pJ28C+n07Py3VfcDmVnRr1c9d4/UpJKoqQgvHU+d6zRTQEA+CCv7uk+cOCAPPzww/LVV19JZKRz5ZwmTJggsbGxliUhwbVSUvCsx/s01EOufZEaSv72kNYy4tJ6Ba736e0dpIyTvfr1KkXnmwPbt2kVPTT936cus9xXs3yUrg3rSK/GlcWX3NejrtFNQEnx0vjUS5sFL0XmcgCA3wbdq1atkmPHjkmbNm0kNDRUL4sWLZJ3331XX8/Jycn3nNGjR0tycrJlUYE7jBMI0yY71YnzyHatE06p3r7wkGDp0aBivvVUKTNVr9yaSuIGeANf7hXOzTXJhN+3yOyN52t5B9LfNdjaQhI1AIC/Bt29evWSDRs2yNq1ay1Lu3btdFI1dT0kJP9w4IiICImJibFZYJyiHpsWpxRXSfvg1nZu3V7nuuXtJpNSy6e3t5fhnWvmS9oWlqcEWp2K+Yesu6JKjO3Ikg61XT+xMOWWtgU8StQC51jX8Pa0di/+KbWe+k0WbDumb/+x8ah88Pduue/LVQUme4N/S8vIlj0n0/R1eroBAH4XdEdHR0uzZs1sltKlS0v58uX1dfgvFbCp+c6erN/tLrGlwuTyJsUf3r34/y6Vtwa3lJs7Xiyh9syVjXXQoeqZ583kbE/0hWHtrhwYVo6JKLR3MiLPEHhnlIu6mEwu3/aJXwKGkd+1qg/+3M+bnG7XidRMfXn7J+dziBxNOWd5rMVzczzVTHi5rUdT9O9F/a2sUCb/30sAAHw+kRp8W1HL8jSsEi2/PthdfIU7OuMS4qL0Yk0lZLutay2bnuy40o4P+lY821sysnNtsqcXJjQ4f0D9wKX1ZOysi8HKbV1qyeIdJ8QVIQ7KqcH/vTNvhyzecVwaV42RkZcVnAOhqFT970kLdkqfplUcrmOuD/7cNU2L/fcrLTP/dCYEBupzAwACLuheuHCh0U0ASnQIbN6h43dfUlv3vKihr/YysLtS2szRiZFbO9W0BN1d65XXidpUz7W97NSOtKlBibNAdCotU96at11fX7nvtJSJDHX7/62HvlkjP687rK9PXrgr4Oaao2SRRA0A4NfDywFf0aRqyZWRiQoPlckFzpd2jb3QQw1j/+Ph7nJvjzryv5tdf61ZD3TV2d5deU34h+ycXJvbRQ2Klcxs220pq/eftgTcnrRs90l58bctbh3BA1/v6aZcGAAgQHq64VsC5dhUBaeqR+6yRpXE17x0bTPZcuSMvDZnm839amiwWhxpXi1WNhxKtvtYy4Syhf4uWtcoK2v2F69W93Wtq8mPaw4Vaxtwr8LyDrjitw2HJfVcts191/3vP5e3o4JkV9s1bNpyl18H/nkSaevRM/p6U3q6AQBFRE83PKpNTe8eYlyUHit7h+5qSPdDvepLs2ol3xPiSiihkrQ9aDXHdtzVTeSyRpX1HO7lz/TS5cd+uL+LU9tSJxjy1hgviMq8bs0dSfIqRkdIVLhrw+nhWe6cyv/ojHUyxiq3QFGpPAf7T6bL8TMZlvsK+6+vngPsPpGmfwulw0OkRp6cGwAAOIugGx41omddebZ/Y5n/WI8Sfd2a5c8fHPVu7Hs9z+7y20Pd5KoWVfOVI3vsioaW29bzvytFR8qbN7aStk6eKFEdh64EWNY1xtV8WlUWblDb6lIcATKQwqd8v+qgeBtVa/uS1xZI+5fmuWUUTqCM4MHFoeVq1E9BU2YAACgIw8vhUSqou6t7nRJ/3e/u6yzztxyTAa3iJVA1jY+V/s2ryq8XMji7mwo8glzoZ7ce3uuuoIW5td7n7Xk7xNt8tmSfW7fHry5wkEQNAOAOBN3wS6rXdmiHi/Wu3am42ZhLUr1KZQp8vEPtuGJt39Xpu2o4ukqO5a4D2GBPpo1HkZzN8v7SWueycuRQ0lmjmwEfsOnw+bwVzOcGABSH70QPgJdQQ6IXbjsul9SvIN6ufuVo+fyODpakUOaa3GvHXi4n0zKlbsWCg/LCuBryqozoaRnZltfNyTV5TdIuBIbP/tura3wDzoykIXM5AMAdCLoR0CpER0hKnuzIhYkIDZGpw9qJr7ikQUW5u3ttWXcgWXpdmOOu5lOrpbisg96wkCDJyik4iM4b5J/NLF6vKFMs4apxP2+SVoVk1y8MsxoCw9GUc3I6PUtCgoOkfuXinaAEAAQ2EqkhoH14a1tpX6ucfHlnR/Fnz/RvIt/e11nCQtz7X/7twa30ZUxkqNQqX9rl5zeqGu3wsdoVCt+eOhgGXLX2QPFK1SEwmHu561UsY5N0EgAAVxF0I6DVqxQt393XRbr5wFBxo4dYN6ySP0Du3aSybB3fV9Y/18fl+d3KfT3qyuNXNLD7WGhwkAxul1Ds9/7Pk5e63jAAAW/ThaCb+dwAgOIi6AZ8XEll8H57cGu7QbC5B8iVTObWzx15WX27j6l39XT/xnJH19ryy8hudtdpUkBPuTLhuuZSvZzj2rrVy5WSoR0SpE5F13vpEbhUyTv4P8t8boJuAEAxEXQDBuhc1/d61qvERsqrg1pYbucNO+x1Oqs67cqQ9gX3WDsSWypMxl7dRJpXt5/EqE/TKgU+Pzsnt8DHVdA94boW0tUHvw8YhzndAVYurCpBNwCgeEikBpSgFc/0loOn06V1jXJu26Y3Z/B+/IqGcmXzqtLIztD04ooKDyn0vReW2M16W4CzvPi/HNwk5VyW7D+Vrq/T0w0AKC56uoESVDE6wq0Bt1LHiYRjJcFeABwcHCTNqsVKqBsSuPVqVMn+6xbwnOzcgnu6zUPio8I5/wjnFWUqBXzLlgtDy6uVLeWWSg8AgMDGkSbgo/56rIcuZ5MQ53jOcknydBjy0fB2ur5y8+fm2gzxLagvO9vJOuBxpcPEW2rAf7/qoNHNQCHo6Q6coeWNGVoOAHADeroBH1WnYhlpW9O9vebF4Y4Mvz+O6FJgT3p0pGvBcXYhw8vNwdMNhWRJLymj+zUyuglwwudL9hrdBHgYSdQAAO5ETzcAt3j2qiZSNipMBrSq5vJzfx7ZVQ4nnZM2bh56b06k9s3dnWTW2kOyLfGMrNmflC/oVlnU1fD1+VuPufX1O9SKk+V7TxW63jtDWkml6EgpXyZCWlaPlXUHk93aDrjXidRMo5sADyOJGgDAnQi6AbiFyjT+TP8mRXpui+plpUX1or1uQSN9sy4ML+9ct7xezmbmSOOxsy2Pq/maZq1rlHV70H3PJXXkvZtay+VvLpKUc9kO1yvKiQoAnpGZnSvbE8/o69ToBgC4A8PLAfhk3SRzrWSTCyXDSuXJUj66X2PL9bsvqSPP9m8sfz56iVvbWTkmssBa4Xld37aIZx8M0q1e4JVbi47kfLU/23ksVVc+UN+zKisIAEBxEXQD8FsFlQy7vk11KVf6YlbiiNAQuat7Half2f3lzQrSv0VVm9u3dKwpM+7pJL7CfPIjkIS7IRs/fGNouTeXZAQA+A6OHAAUjYcPRk1u6EkvqGRYSRxLd6pbvtB1+jevmq/MWsc6hT/PWzSuEnjDb0deVs/oJsCDSKIGAHA3gm4Avjm83OREne4CerojQoNdKuXlqvt61JUyEf4/DPnRyxtIoPGWbPfwjE2HzycybBofa3RTAAB+gqAbgN8xzzO+qWONfI+NuaqJNKhcRh7uXd/h84d3rmm5flWLqvL6DS1dev2HetWXp6zKfxXUq27wuYtiKx0AJxbyCmbEsd9SI2zIXA4AcLfAO1oC4Pc+u6ODnE7PlAplIvI9dme32nopiPU8zvdvauPy67vSi16/chlxt7CQIHlnSGsZ8dVqt28bKugm6vZXB0+flTPnsvX/oXqV3P9/EwAQmOjpBuBT7r2kjqXH2pGQ4CC7AbezOtWJE3dy1JutEnI18EDiNhUU+noPujcj5vZf5l7u+pWiJdyFk2cAABSEnm4ARWIyaPtq2PYd3WrrUlye0qdpFZlyS5sC53SqBGi/bThSrPfSvb5nym2pkw7wHHq6/demC0nUqM8NAHAnTuMC8Clq6LcnA27za/RtVlUS4vLX1x7dr5FsfL6PvDW4lUxycui5dSb2NWMut3odNzVYRLa80NdyPSQoiN5YD+Kj9V9kLgcAeAJBN4AiCdTAIywkWGclV0NP89bYdsR6qLd1bXB3foqlwkNsyo65c3j5y9c2d9/G/AA93f5rC0nUAAAeQNANoEi8ZcrwvT3qirczOfi0Ql0YBv7zyK6FrtO+Vjl9eYOdEmeuvFZetSuUtrldocz5Ewd1KtreHyj8PeaeNGmS1KpVSyIjI6Vjx46yfPlyh+tOnTpVunfvLuXKldNL7969C1z/vvvu0yNJ3n77bfE2SemZcijprL7emJ5uAIAbEXQD8GkjL61nc7tqrGeHnheFo15nV+Zet6hettB1Pr6tvUwd1k6e6NswX6DfqU55m6ztKsN7Uf3yYDe5t0cd+eLOjhKIrLPb+5sZM2bIqFGjZNy4cbJ69Wpp2bKl9OnTR44dO2Z3/YULF8rQoUNlwYIFsmTJEklISJArrrhCDh06lG/dmTNnytKlSyU+Pl68eWh5jbgoiYkMM7o5AAA/QtANwKepodTWvrzLs4FgywTHydXM8sZkJifb7sijvRvY3G5UJVquaZk/cFGBwuVNKktE6MWh5tZUoFwpOkLu71lXD5F3Vt73UzW2lIzu11iqlS3l9DbgG9588025++675fbbb5cmTZrIlClTJCoqSqZNm2Z3/a+++kpGjBghrVq1kkaNGslHH30kubm5Mn/+fJv1VBD+4IMP6vXDwrwzoKU+NwDAU8heDsCv1K3omdq6i57oKftOpkvbmvbLiVWMjpDjZzIKTaRmLcTJDtPq5Urlm1P87tDWsmzPSUlMsf+a9gLnStGRsuzpXrqn9mSqc8+zHk4O/5aZmSmrVq2S0aNHW+4LDg7WQ8ZVL7Yz0tPTJSsrS+LiLv4/UUH4rbfeKk888YQ0bdrUqe1kZGToxSwl5XxA7EkkUQMAeAo93QDghJrlS8slDSo6fDwuKrwIw8uL9ifYHIQXJVmaeWh0eRfqmNer5P5a4vA+J06ckJycHKlcubLN/er20aNHndrGk08+qYePq0Dd7NVXX5XQ0FB56KGHnG7LhAkTJDY21rKoYeueRk83AMBTCLoBuOSqCxm7h3aoERiZ2gqh5ka3rVlOJt18sXxY3mD4xWub6cvHLrcdJh4d6dpgo+n3dJKrW8ZbtucjH5FLvyv4rldeeUWmT5+u526rJGyK6jl/55135NNPP3VpLrzqbU9OTrYsBw4c8GDLRc5l5ciOY6n6etNqBN0AAPdieDkAl7w3tLW8cWNLh/OGA02PBhX1UpAudSvIthf7Wj6zl65tJj+sOigP96rv0mupZGhqMXNnWTCzdjXLycp9p/Pd37pGWVmzP0nPCfeEN29sJb+uP+KRbcM5FSpUkJCQEElMTLS5X92uUqVKgc99/fXXddA9b948adGiheX+xYsX6yRsNWpcPEmnetMfe+wxncF87969drcXERGhl5KyIzFVcnJNUi4qTKrEeF8yRgCAb6OnG4BLVG8VAXfB7HXoWX9mN3esKT+O6JqnZndROI668wbkzvYyvjO0te517tnQ9kTClFvayt3da8t393UWT1B1z/96rAfJ2QwUHh4ubdu2tUmCZk6K1rmz4+994sSJMn78eJk9e7a0a9fO5jE1l3v9+vWydu1ay6KGn6v53XPmzBFvsflIsmU+tz9npwcAGIOebgCAhQp637+pjaw/mCQLtx2X6AtZzivHRMoz/Zt49LXrVCwj/z51mdR66jePvg4cU+XChg8froPnDh066N7otLQ0nc1cGTZsmFSrVk3PuTbP1x47dqx8/fXXura3ee53mTJl9FK+fHm9WFPZy1XPecOGDcVbWJKoMZ8bAOABBN0AfN7Hw9vJiK9Wy2s3tJRAUtDw8rydda723am64HMfvUSqeGHdc3jO4MGD5fjx4zqQVgG0KgWmerDNydX279+vM5qbTZ48WWc9HzRokM12VJ3v5557TnzFpgtBd9P4wksCAgDgKoJuAD6vV+PKsvmFvhLiZN1rf/HApfXkhV83y8BW+Wt2925cWWqVj5K9J9Od3l5EqO2MowaVPZO1vEOtOFm+95TDxzc8d4Uuv3bZG4uK9TpqiLwqqbblQlZqd+hsNafeX40cOVIv9ixcuNDmtqM52QUpynM8KTfXZPmNUC4MAOAJzOkG4FUGXAggR1xaz6XnBVrArdzetZbMG3WJvHFjq3yPRYaFyILHe1puOzNNtfSFoeRGi44Mk+rlooq9HfWW40qHiTt9OKytW7cH4+0/lS5pmTk6r0CdCqWNbg4AwA8RdAPwKm/d2EoW/9+lMqhtdaOb4vVUwidVQ9vRCQf1+BN9GurSZM/2b5zv8ebVbIfSmoqYDt2chb1rvfJ2e9zzMjlR7EwFQAUZ2iFB7u9Zt8B1asQVP3C3d0IA/sVcn7tRlWgJDeGwCADgfuxdAHiV4OAgSfBAsBSo1BD0dWOv0MF5XrMe6CpjrrqYHK2oFcgevbyBrB17uXx1V6d8j31wa/6eYWdj+4Iymatgvn9z+7W9v7m7kwxpnyCP9fGeRF3wXpsOn89c3pSh5QAAD/GOsYQAAI+eyHB0f6mwi6XMgotRKqlslP3yZ/Ze2tngvqCe94IC9851y+ulsPUAhczlAABPo6cbAAKYdZw91QPzldUQ9/dvai1VrbKgOzuMvXt921rhgCeHl5NEDQDgKQTdAOCFapYvmSH21h3RbWvGuXXbwzrX1JdXtYiXkZddTIznbOfz2Ksd1wV3tlOenm4U5ERqhs5wr35PjaoQdAMAPIOgGwDcLMjlqtj5qRrZJaFb/Qr6smyU+xOEvTCgmd3PxNlAWGVTb12jrMPH7W1nyi1kF4frQ8trly/tNdn7AQD+h6AbALxQROjFudbFmGpdKFWaa9nTvWTJU71KpBScYj3UvDDWgXW3eudPEBSkb7MqrjcOEuhDyxsztBwA4EEE3QDgZlViI9y6PU8Pka4cEymlwi8G+cVxSQP787CtexFrVygt7w5t7fK28z7HmZMRzpQnQ+AiiRoAoCQQdAOAm3w8vJ3ce0kduaZlNQlU7WqWc2q9a1rGS5QTgf7VLeMtgXpc6XCPnoyoUCZchneuKTe2qy5f3tnRvRuHVyKJGgCgJDCBCQDcpFfjynpxN08OLzdSVk5uoevc1qWW1K1YWlolOJ7bbVbUOstP9Gkol9SvKM2rx7rUNvi2s5k5svt4qr5OjW4AgCfR0w0AKFHmDuqKZQofhh8SHCQ9G1bKVwfc3omIj4a3y3ffeKtkbo48cGk9m4Bbb7/QZ8HXbT2aIrkmNcIhQipFO59nAAAAVxF0AwAM8fFt7aVj7Tj57r7Obtle1dhS+e6rXzna5vagttVl/XNXOFVfHP6NoeUAgJJC0A0AcJvr21bXl90vlCIrSOOqMTLj3s7SvpZ764MX5OVrm0tMZOHl0YKtYu4Glct4tlEwBEnUAAAlhTndAODlVBIxX1GtbCnZ/EIfKRXmnmzojlQsEynlSrteWzw89Py55siwYDmXlSvRDmozW/d0P9K7gUxZtEuub3P+hAL8q6eb+dwAAE8j6AYALzXrga6y71S6tK7hXEZwbxEV7rldy9Rh7eTg6XTLHOy+TavI7E1HXd7O3Ed6yA+rD+pEbYWpFB0hP4/sVqT2wjvl5Jpk65Ez+jrDywEAnkbQDQBeqmVCWb3gosub2GaHv65NtSIF3TXKR8mjlzdwat3QEGZi+Zs9J9LkbFaOHpFRq7zvjCQBAPgmjiQAACXCPIx3QKvztbfdwZMJz3o2rKjnnTejJ9Rvh5Y3qhqtM+QDAOBJ9HQDAErETw90ldPpmW4tz+RMuKRqfK89kCRVYlx73U9ua3/+Nchk7rdJ1JjPDQAoCQTdAIASERYSbEg95A9ubSsf/7NHbulY06XnEWz7r02Hk/Vlk6q29dkBAPAEgm4AgM9yJi6uHBMpT1/ZuCSaAx9gMpkulgujpxsAUAKY0w0A8Fl0RsNVx89kyMm0TF2LvWHlaKObAwAIAATdAAAgYGy6kEStbsUyUircs/XkAQBQCLoBAEDAYGg5AKCkEXQDAHxWkFP5ywE7QXdVgm4AQMkg6AYA+Kz6lcsY3QT4aI1ueroBACWF7OUAAJ9VvVyUzHqgq5SNCjO6KfARLw1sJpsOp0jzapQLAwCUDIJuAIBPa5lQ1ugmwId0qVdBLwAAlBSGlwMAAAAA4CEE3QAAAAAAeAhBNwAAAAAAHkLQDQAAAACAhxB0AwAAAADgIQTdAAAAAAB4CEE3AAAAAAAeQtANAAAAAICHEHQDAAAAABCIQfeECROkffv2Eh0dLZUqVZKBAwfKtm3bjG4WAAAAAAC+H3QvWrRIHnjgAVm6dKn8+eefkpWVJVdccYWkpaUZ3TQAAAAAAAoVKl5s9uzZNrc//fRT3eO9atUqueSSSwxrFwAAAAAAPh9055WcnKwv4+LiHK6TkZGhF7OUlJQSaRsAAAAAAD41vNxabm6uPPLII9K1a1dp1qxZgfPAY2NjLUtCQkKJthMAAAAAAJ8LutXc7o0bN8r06dMLXG/06NG6R9y8HDhwoMTaCAAAAACAzw0vHzlypPz666/y999/S/Xq1QtcNyIiQi9mJpNJXzLMHADgTcz7JfN+Co6xLwcA+PK+3KuDbtX4Bx98UGbOnCkLFy6U2rVru7yNM2fO6EuGmQMAvJHaT6npUHCMfTkAwJf35UEmLz7FPmLECPn6669l1qxZ0rBhQ8v96g2VKlXK6bnghw8f1rW+g4KCin0mQ+3w1ZD1mJgY8TW031i031i031i0Pz+1+1U76fj4eAkO9pnZXoZgX24f78X7+Mv7UHgv3slf3kuKn7wPZ/flXt3TPXnyZH3Zs2dPm/s/+eQTue2225zahnrzhQ1Jd5X6Yfjyj4P2G4v2G4v2G4v226KH2znsywvGe/E+/vI+FN6Ld/KX9xLjB+/DmX25VwfdXtwJDwAAAABAoRjPBgAAAACAhxB0u0BlRR83bpxNdnRfQvuNRfuNRfuNRfvhLfzpu+S9eB9/eR8K78U7+ct7ifCT9+Esr06kBgAAAACAL6OnGwAAAAAADyHoBgAAAADAQwi6AQAAAADwEIJuF0yaNElq1aolkZGR0rFjR1m+fLnRTZLnnntOgoKCbJZGjRpZHj937pw88MADUr58eSlTpoxcf/31kpiYaLON/fv3S//+/SUqKkoqVaokTzzxhGRnZ3ukvX///bdcffXVuoC8autPP/1k87hKMTB27FipWrWqlCpVSnr37i07duywWefUqVNy880365p+ZcuWlTvvvFNSU1Nt1lm/fr10795df1cJCQkyceLEEmm/qh+f9/vo27ev17R/woQJ0r59e4mOjtbf9cCBA2Xbtm0267jrN7Nw4UJp06aNTpBRr149+fTTTz3e9p49e+b7/O+77z7D265MnjxZWrRoYalH2blzZ/njjz+8/nN3tv3e/Nnb88orr+g2PvLIIz7zHcBz++vvvvtO7zvV+s2bN5fff/9djObM37y81G8t7/9D9Z68/VjFV74TRf2u8r4Xtai/Hd78nbjj+Mtbjo0Lei9ZWVny5JNP6t9M6dKl9TrDhg2Tw4cPu/036un34uxxpTd8L4W9jyA7/2fU8tprr3ndd+IxKpEaCjd9+nRTeHi4adq0aaZNmzaZ7r77blPZsmVNiYmJhrZr3LhxpqZNm5qOHDliWY4fP255/L777jMlJCSY5s+fb1q5cqWpU6dOpi5dulgez87ONjVr1szUu3dv05o1a0y///67qUKFCqbRo0d7pL1q+88884zpxx9/VAn8TDNnzrR5/JVXXjHFxsaafvrpJ9O6detM11xzjal27dqms2fPWtbp27evqWXLlqalS5eaFi9ebKpXr55p6NChlseTk5NNlStXNt18882mjRs3mr755htTqVKlTB988IHH2z98+HDdPuvv49SpUzbrGNn+Pn36mD755BO93bVr15quvPJKU40aNUypqalu/c3s3r3bFBUVZRo1apRp8+bNpvfee88UEhJimj17tkfb3qNHD/1/0/rzV5+n0W1Xfv75Z9Nvv/1m2r59u2nbtm2mp59+2hQWFqbfjzd/7s6235s/+7yWL19uqlWrlqlFixamhx9+2HK/t38H8Mz++t9//9Xf0cSJE/V39uyzz+rf9oYNG0xGcuZvXl5q/ZiYGJv/h0ePHjUZrbBjFV/5TpRjx47ZvI8///xTHw8sWLDAq78Tdxx/ecuxcUHvJSkpSf+NnjFjhmnr1q2mJUuWmDp06GBq27atW3+jJfFenD2u9IbvpbD3ccSq/WpRbQsKCjLt2rXL674TTyHodpL6D/vAAw9Ybufk5Jji4+NNEyZMMLRd6gepAjh71B8etZP67rvvLPdt2bJF/2dQf4TM/0mCg4NtdgCTJ0/WO4iMjAyPtj3vf8rc3FxTlSpVTK+99prNe4iIiNCBp6J2vup5K1assKzzxx9/6P+4hw4d0rf/97//mcqVK2fT/ieffNLUsGFDj7bf/MdxwIABDp/jTe03Hzyo9ixatMitv5n/+7//038orQ0ePFgfRHqq7ebAzzqIystb2m6mvuePPvrIpz53e+33pc/+zJkzpvr16+sDZes2++p3gOLvr2+88UZT//79be7r2LGj6d577zV5E3t/8+wFeCpw8jYFHav48neiqL8hdevW1ccwvvKdFOX4y1uPje0di9k70arW27dvn9t+o55QlONKb/xenPlOBgwYYLrssssKXMcbvhN3Yni5EzIzM2XVqlV6qI1ZcHCwvr1kyRIxmhr+o4Zz1KlTRw9bVsMfFdVmNczGut1qWEaNGjUs7VaXaghO5cqVLev06dNHUlJSZNOmTSX6Pvbs2SNHjx61aW9sbKweFmPdXjUku127dpZ11Prq+1i2bJllnUsuuUTCw8Nt3pMalnf69GmPvw81tFQNBWzYsKHcf//9cvLkSctj3tb+5ORkfRkXF+fW34xax3ob5nXc+f8lb9vNvvrqK6lQoYI0a9ZMRo8eLenp6ZbHvKXtOTk5Mn36dElLS9PDtH3pc7fXfl/67NUQUDU8PO/r+Np3APftr33lO3P0Ny8vNV2pZs2aemrSgAEDSnxf7uqxii9/J+r39uWXX8odd9yhh7762nfiyvGXrx0b5/2/o74fdfzlrt9oSSrouNIXv5fExET57bff9PTKwnjrd1IUoUY3wBecOHFCH2RaH2gp6vbWrVvFSOoPopovpP4jHjlyRJ5//nk9F3jjxo36D6gK3PL+kVHtVo8p6tLe+zI/VpLMr2evPdbtVX94rIWGhuqDEOt1ateunW8b5sfKlSvnsfeg5tlcd911+vV37dolTz/9tPTr10//oQsJCfGq9ufm5ur5rF27dtVBknn77vjNOFpHBSdnz57V88Xc3Xblpptu0gc26g+0mhev5nWpkxU//vijV7R9w4YNOkhVc4fVnOGZM2dKkyZNZO3atT7xuTtqvy989oo6UbB69WpZsWJFvsd85bcP9++vHX1nJb0PLMrfvLzUscC0adN0/gUVaLz++uvSpUsXHeRVr15dvPFYRc1Z98XvRFHzVpOSkvS8W1/7Tlw9/vKlY2Nran+l9kdDhw7V+Ujc9RstKYUdV/ri9/LZZ5/pz1S9r4J463dSVATdPk79xzNTf9DVD1Qd+H777bcc3BlgyJAhluuqR0x9J3Xr1tVnKXv16iXeRPX4qT9c//zzj/gaR22/5557bD5/lRBGfe5qR6W+B6OpHYcKsNWB1/fffy/Dhw+XRYsWia9w1H4VeHv7Z3/gwAF5+OGH5c8///SKxFKAJ/5eq5Ni1qNPVHDXuHFj+eCDD2T8+PHijccqzvR2eauPP/5Yvzd1stHXvpNAoEYv3XjjjTpJnEoG6ou/UV86rnTWtGnTdK91Yftib/1Oiorh5U5QwyXV2aS8WWzV7SpVqog3Ub00DRo0kJ07d+q2qWEm6iyso3arS3vvy/xYSTK/XkGfs7o8duyYzeMqc7DKCO6N70kNh1G/H/V9eFP7R44cKb/++qssWLDA5ky7u34zjtZRZ5mLezLIUdvtUX+gFevP38i2q55Ulc26bdu2OjNxy5Yt5Z133vGJz72g9vvCZ6+G26n/eyqruBpdohZ1wuDdd9/V11UvgC98B3D//trRd+Yt+3dX/ublFRYWJq1bt7b8P/TGYxVf/E6Uffv2ybx58+Suu+7y+e/EmeMvXzs2Ngfc6ntSJ1sL6uUuym/UKHmPK33te1m8eLEeBefq/xtv/k6cRdDt5IGmOsicP3++zVAvddv67KU3UPOGVM+S6mVSbVZ/3K3brX7oaj6Eud3qUg0ZtQ4EzX+czMNGS4oaOqP+IFi3Vw3JVHOdrdurDorVAbTZX3/9pb8P80G+WkeVLlB/cK3fk+ql8+TQcnsOHjyo596o78Mb2q/O9qoDODUsWL1u3mHs7vrNqHWst2Fepzj/Xwpruz2qV1ax/vyNaLsj6nvPyMjw6s/dmfb7wmevegTU66t2mReVW0GdbTdf98XvAMXfX3vrd1aUv3l5qWGm6jdr/n/ojccqvvSdWPvkk0/0dDGVI8LXvxNnjr986djYHHCr+cDqxIgqA+nu36hR8h5X+tL3Yh4dotqnTtr7y3fiNKMzufkKlX5fZXH89NNPdQbqe+65R6ffN7oUx2OPPWZauHChac+ePbrEhiqToErYqCyn5hI4qsTIX3/9pUvgdO7cWS95S+BcccUVuiSJKmtTsWJFj5UMU5mDVakdtaif35tvvqmvmzNKqpIV6nOdNWuWaf369Tq7ob2SYa1btzYtW7bM9M8//+hMxNYlt1TGTVVy69Zbb9WlVtR3p0r4uKPkVkHtV489/vjjOtOx+j7mzZtnatOmjW7fuXPnvKL9999/v86iqn4z1iUY0tPTLeu44zdjLpv0xBNP6AzQkyZNKnbZpMLavnPnTtMLL7yg26w+f/UbqlOnjumSSy4xvO3KU089pbMOq7ap37a6rbLWz50716s/d2fa7+2fvSN5M657+3cA9+yv1d9W9fs1U/vO0NBQ0+uvv66/M5Ux1xvKUznz9zrve3n++edNc+bM0WV4Vq1aZRoyZIgpMjJSlw3y5mMVX/lOrLNBq78VqrJIXt76nbjj+Etlm1ZlEI0+Ni7ovWRmZupyZ9WrV9d/p63/71hXhcn7Xgr7jRrxXpw9rvSG76Ww35eiyoiq/aOq+mGPt3wnnkLQ7QL1Q1B/ZFXtO5WOX9VZNpoqRVO1alXdpmrVqunb6gDYTP2xHDFihC7to37o1157rf7DY23v3r2mfv366VrQ6sesfuRZWVkeaa+qY6n+M+ZdVEkEc9mKMWPG6KBT/cHo1auXrgls7eTJkzpILVOmjC7Vc/vtt+v/7NZUjclu3brpbajPRe1MPN1+dSCkDsbVQbg6OKhZs6aujZj3j5yR7bfXdrWokibu/s2oz6pVq1b6t6kCMOvX8ETb9+/fr4O8uLg4/bmp+ucq8LGuFW1U25U77rhD/ybUNtVvRP22zQG3N3/uzrTf2z97Z4Nub/8O4J79tfrezfscs2+//dbUoEEDvb4q+aZq0hvNmb/Xed/LI488Ynnfaj+qanuvXr3a5O3HKr7ynZipIFp9F3mPT7z5O3HH8ZfaB6gTIEYfGxf0XlSA5uj/jnUt9bzvpbDfqBHvxdnjSm/4Xgr7fSmq80jtO1Xnkj3e8p14SpD6x+jedgAAAAAA/BFzugEAAAAA8BCCbgAAAAAAPISgGwAAAAAADyHoBgAAAADAQwi6AQAAAADwEIJuAAAAAAA8hKAbAAAAAAAPIegGAAAAAMBDCLoBAAAAFCgoKEh++ukno5sB+CSCbiBAHT9+XO6//36pUaOGRERESJUqVaRPnz7y77//6sfZuQIA4B1uu+02vV/Ou/Tt29fopgFwQqgzKwHwP9dff71kZmbKZ599JnXq1JHExESZP3++nDx50uimAQCAPFSA/cknn9jcp06aA/B+9HQDASgpKUkWL14sr776qlx66aVSs2ZN6dChg4wePVquueYaqVWrll7v2muv1WfSzbeVWbNmSZs2bSQyMlIH688//7xkZ2dbHlfrT548Wfr16yelSpXS63z//feGvE8AAPyFeVSa9VKuXDmn970bNmyQyy67TD9evnx5ueeeeyQ1NdVmnWnTpknTpk31a1WtWlVGjhxp8/iJEyf0sUFUVJTUr19ffv755xJ454DvI+gGAlCZMmX0ooaPZ2Rk5Ht8xYoV+lKdUT9y5IjltgrUhw0bJg8//LBs3rxZPvjgA/n000/lpZdesnn+mDFjdE/6unXr5Oabb5YhQ4bIli1bSujdAQAQeAra96alpekpZCpIV/v07777TubNm2cTVKug/YEHHtDBuArQVUBdr149m9dQJ9pvvPFGWb9+vVx55ZX6dU6dOlXi7xXwOSYAAen77783lStXzhQZGWnq0qWLafTo0aZ169ZZHld/HmbOnGnznF69eplefvllm/u++OILU9WqVW2ed99999ms07FjR9P999/vsfcCAIA/Gz58uCkkJMRUunRpm+Wll15yat/74Ycf6n1+amqq5fHffvvNFBwcbDp69Ki+HR8fb3rmmWcctkG9xrPPPmu5rbal7vvjjz/c/n4Bf8OcbiBAqbPh/fv3173XS5culT/++EMmTpwoH330kU7YYo86e64SrVn3bOfk5Mi5c+ckPT1dDzdTOnfubPM8dXvt2rUefkcAAPgvNR1M9UZbi4uLs1wvaN+rerxbtmwppUuXtjzetWtXyc3NlW3btunh6YcPH5ZevXoV2IYWLVpYrqttxcTEyLFjx4r93gB/R9ANBDA1L/vyyy/XixqWdtddd8m4ceMcBt1q7pcaWnbdddfZ3RYAAPAMFeTmHe7tLmqetzPCwsJsbqtgXQXuAArGnG4AFk2aNNHzvsw7VtWLbU0lUFNnxNVOP+8SHHzxz4nqObembjdu3LiE3gUAAIGnoH2vulSj1cz7eEWNXFP77oYNG0p0dLROmqqqmABwP3q6gQCkyoLdcMMNcscdd+ihYmpnu3LlSj28fMCAAXod885XDT9TWUxV8pWxY8fKVVddpWt7Dxo0SO+s1U5848aN8uKLL1q2rxK0tGvXTrp16yZfffWVLF++XD7++GMD3zEAAL5NJT49evSozX2hoaFSoUKFQve9KuGZGsk2fPhwee655+T48ePy4IMPyq233iqVK1fW66j777vvPqlUqZLOgn7mzBkdmKv1ABQPQTcQgFTm8o4dO8pbb70lu3btkqysLElISJC7775bnn76ab3OG2+8IaNGjZKpU6dKtWrVZO/evTrz6a+//iovvPCCLjemesMbNWqkh6VbU0PQp0+fLiNGjNAlR7755hvdiw4AAIpm9uzZep9qTfVSb926tdB9r8q5MmfOHF19pH379vq2yu3y5ptvWralAnKVo0UdGzz++OM6mFcn2AEUX5DKpuaG7QCAZX7XzJkzZeDAgUY3BQCAgMC+F/BuzOkGAAAAAMBDCLoBAAAAAPAQhpcDAAAAAOAh9HQDAAAAAOAhBN0AAAAAAHgIQTcAAAAAAB5C0A0AAAAAgIcQdAMAAAAA4CEE3QAAAAAAeAhBNwAAAAAAHkLQDQAAAACAhxB0AwAAAAAgnvH/+RzaFRJTnY8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trainer.train_losses)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(trainer.accuracies)\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4️⃣ Greedy Sampling from a Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jingle bells, jingle bells, jingle all the way of the best of the best of the\n"
     ]
    }
   ],
   "source": [
    "class TransformerSampler:\n",
    "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def sample(\n",
    "        self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an end-of-sequence token. kwargs are\n",
    "        passed to sample_next_token, to give detailed instructions on how new tokens are chosen.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
    "\n",
    "        for _ in range(max_tokens_generated):\n",
    "            # Get new logits (make sure we don't pass in more tokens than the model's context length)\n",
    "            logits = self.model(input_ids[None, -self.cfg.n_ctx :])\n",
    "            # We only take logits for the last token, because this is what we're sampling\n",
    "            if hasattr(logits, \"logits\"):\n",
    "                logits = logits.logits\n",
    "            logits = logits[0, -1]\n",
    "            # Get next token (as a tensor of size (1, 1) so we can concat it to input_ids)\n",
    "            next_token = t.tensor(\n",
    "                [TransformerSampler.sample_next_token(input_ids, logits, **kwargs)],\n",
    "                device=device,\n",
    "            )\n",
    "            # Create new input ids string, with shape (1, old_seq_len + 1)\n",
    "            input_ids = t.cat([input_ids, next_token], dim=-1)\n",
    "            # Print out results, if required\n",
    "            if verbose:\n",
    "                print(self.tokenizer.decode(input_ids), end=\"\\r\")\n",
    "            # If our new token was the end-of-text token, stop\n",
    "            if next_token == getattr(self.tokenizer, \"eos_token_id\", None):\n",
    "                break\n",
    "\n",
    "        return self.tokenizer.decode(input_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "    ) -> int:\n",
    "        return TransformerSampler.greedy_search(logits)\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the most likely token (as an int).\n",
    "        \"\"\"\n",
    "        return logits.argmax().item()\n",
    "\n",
    "\n",
    "t.set_grad_enabled(False)  # gradients are not necessary for sampling\n",
    "\n",
    "model.eval()\n",
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=8)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2.cfg = model.cfg\n",
    "sampler = TransformerSampler(reference_gpt2, tokenizer)\n",
    "\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=8)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
